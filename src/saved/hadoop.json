[
    {
        "id": 1,
        "Question": "IBM and ________ have announced a major initiative to use Hadoop to support university courses in distributed computer programming.",
        "Options": [
            "a) Google Latitude",
            "b) Android (operating system)",
            "c) Google Variations",
            "d) Google",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 2,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop is an ideal environment for extracting and transforming small volumes of data",
            "b) Hadoop stores data in HDFS and supports data compression/decompression",
            "c) The Giraph framework is less  useful than a MapReduce job to solve graph and machine learning",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 3,
        "Question": "What license is Hadoop distributed under?",
        "Options": [
            "a) Apache License 2.0",
            "b) Mozilla Public License",
            "c) Shareware",
            "d) Commercial",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 4,
        "Question": "Sun also has the Hadoop Live CD ________ project, which allows running a fully functional Hadoop cluster using a live CD.",
        "Options": [
            "a) OpenOffice.org",
            "b) OpenSolaris",
            "c) GNU",
            "d) Linux",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 5,
        "Question": "Which of the following genres does Hadoop produce?",
        "Options": [
            "a) Distributed file system",
            "b) JAX-RS",
            "c) Java Message Service",
            "d) Relational Database Management System",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 6,
        "Question": "What was Hadoop written in?",
        "Options": [
            "a) Java (software platform)",
            "b) Perl",
            "c) Java (programming language)",
            "d) Lua (programming language)",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 7,
        "Question": "Which of the following platforms does Hadoop run on?",
        "Options": [
            "a) Bare metal",
            "b) Debian",
            "c) Cross-platform",
            "d) Unix-like",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 8,
        "Question": "Hadoop achieves reliability by replicating the data across multiple hosts and hence does not require ________ storage on hosts.",
        "Options": [
            "a) RAID",
            "b) Standard RAID levels",
            "c) ZFS",
            "d) Operating system",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 9,
        "Question": "Above the file systems comes the ________ engine, which consists of one Job Tracker, to which client applications submit MapReduce jobs.",
        "Options": [
            "a) MapReduce",
            "b) Google",
            "c) Functional programming",
            "d) Facebook",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 10,
        "Question": "The Hadoop list includes the HBase database, the Apache Mahout ________ system, and matrix operations.",
        "Options": [
            "a) Machine learning",
            "b) Pattern recognition",
            "c) Statistical classification",
            "d) Artificial intelligence",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 11,
        "Question": "As companies move past the experimental phase with Hadoop, many cite the need for additional capabilities, including _______________",
        "Options": [
            "a) Improved data storage and information retrieval",
            "b) Improved extract, transform and load features for data integration",
            "c) Improved data warehousing functionality",
            "d) Improved security, workload management, and SQL support",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 12,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop do need specialized hardware to process the data",
            "b) Hadoop 2.0 allows live stream processing of real-time data",
            "c) In the Hadoop programming framework output files are divided into lines or records",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 13,
        "Question": "According to analysts, for what can traditional IT systems provide a foundation when they’re integrated with big data technologies like Hadoop?",
        "Options": [
            "a) Big data management and data mining",
            "b) Data warehousing and business intelligence",
            "c) Management of Hadoop clusters",
            "d) Collecting and storing unstructured data",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 14,
        "Question": "Hadoop is a framework that works with a variety of related tools. Common cohorts include ____________",
        "Options": [
            "a) MapReduce, Hive and HBase",
            "b) MapReduce, MySQL and Google Apps",
            "c) MapReduce, Hummer and Iguana",
            "d) MapReduce, Heron and Trumpet",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 15,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hardtop processing capabilities are huge and its real advantage lies in the ability to process terabytes & petabytes of data",
            "b) Hadoop uses a programming model called “MapReduce”, all the programs should conform to this model in order to work on the Hadoop platform",
            "c) The programming model, MapReduce, used by Hadoop is difficult to write and test",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 16,
        "Question": "What was Hadoop named after?",
        "Options": [
            "a) Creator Doug Cutting’s favorite circus act",
            "b) Cutting’s high school rock band",
            "c) The toy elephant of Cutting’s son",
            "d) A sound Cutting’s laptop made during Hadoop development",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 17,
        "Question": "All of the following accurately describe Hadoop, EXCEPT ____________",
        "Options": [
            "a) Open-source",
            "b) Real-time",
            "c) Java-based",
            "d) Distributed computing approach",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 18,
        "Question": "__________ can best be described as a programming model used to develop Hadoop-based applications that can process massive amounts of data.",
        "Options": [
            "a) MapReduce",
            "b) Mahout",
            "c) Oozie",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 19,
        "Question": "__________ has the world’s largest Hadoop cluster.",
        "Options": [
            "a) Apple",
            "b) Datamatics",
            "c) Facebook",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 20,
        "Question": "Facebook Tackles Big Data With _______ based on Hadoop.",
        "Options": [
            "a) ‘Project Prism’",
            "b) ‘Prism’",
            "c) ‘Project Big’",
            "d) ‘Project Data’",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 21,
        "Question": "________ is a platform for constructing data flows for extract, transform, and load (ETL) processing and analysis of large datasets.",
        "Options": [
            "a) Pig Latin",
            "b) Oozie",
            "c) Pig",
            "d) Hive",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 22,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hive is not a relational database, but a query engine that supports the parts of SQL specific to querying data",
            "b) Hive is  a relational database with SQL support",
            "c) Pig is a relational database with SQL support",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 23,
        "Question": "_________ hides the limitations of Java behind a powerful and concise Clojure API for Cascading.",
        "Options": [
            "a) Scalding",
            "b) HCatalog",
            "c) Cascalog",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 24,
        "Question": "Hive also support custom extensions written in ____________",
        "Options": [
            "a) C#",
            "b) Java",
            "c) C",
            "d) C++",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 25,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Elastic MapReduce (EMR) is Facebook’s packaged Hadoop offering",
            "b) Amazon Web Service Elastic MapReduce (EMR) is Amazon’s packaged Hadoop offering",
            "c) Scalding is a Scala API on top of Cascading that removes most Java boilerplate",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 26,
        "Question": "________ is the most popular high-level Java API in Hadoop Ecosystem",
        "Options": [
            "a) Scalding",
            "b) HCatalog",
            "c) Cascalog",
            "d) Cascading",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 27,
        "Question": "___________ is general-purpose computing model and runtime system for distributed data analytics.",
        "Options": [
            "a) Mapreduce",
            "b) Drill",
            "c) Oozie",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 28,
        "Question": "The Pig Latin scripting language is not only a higher-level data flow language but also has operators similar to ____________",
        "Options": [
            "a) SQL",
            "b) JSON",
            "c) XML",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 29,
        "Question": "_______ jobs are optimized for scalability but not latency.",
        "Options": [
            "a) Mapreduce",
            "b) Drill",
            "c) Oozie",
            "d) Hive",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 30,
        "Question": "______ is a framework for performing remote procedure calls and data serialization.",
        "Options": [
            "a) Drill",
            "b) BigTop",
            "c) Avro",
            "d) Chukwa",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 31,
        "Question": "A ________ node acts as the Slave and is responsible for executing a Task assigned to it by the JobTracker.",
        "Options": [
            "a) MapReduce",
            "b) Mapper",
            "c) TaskTracker",
            "d) JobTracker",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 32,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) MapReduce tries to place the data and the compute as close as possible",
            "b) Map Task in MapReduce is performed using the Mapper() function",
            "c) Reduce Task in MapReduce is performed using the Map() function",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 33,
        "Question": "___________ part of the MapReduce is responsible for processing one or more chunks of data and producing the output results.",
        "Options": [
            "a) Maptask",
            "b) Mapper",
            "c) Task execution",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 34,
        "Question": "_________ function is responsible for consolidating the results produced by each of the Map() functions/tasks.",
        "Options": [
            "a) Reduce",
            "b) Map",
            "c) Reducer",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 35,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner",
            "b) The MapReduce framework operates exclusively on <key, value> pairs",
            "c) Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 36,
        "Question": "Although the Hadoop framework is implemented in Java, MapReduce applications need not be written in ____________",
        "Options": [
            "a) Java",
            "b) C",
            "c) C#",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 37,
        "Question": "________ is a utility which allows users to create and run jobs with any executables as the mapper and/or the reducer.",
        "Options": [
            "a) Hadoop Strdata",
            "b) Hadoop Streaming",
            "c) Hadoop Stream",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 38,
        "Question": "__________ maps input key/value pairs to a set of intermediate key/value pairs.",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Both Mapper and Reducer",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 39,
        "Question": "The number of maps is usually driven by the total size of ____________",
        "Options": [
            "a) inputs",
            "b) outputs",
            "c) tasks",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 40,
        "Question": "_________ is the default Partitioner for partitioning key space.",
        "Options": [
            "a) HashPar",
            "b) Partitioner",
            "c) HashPartitioner",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 41,
        "Question": "Running a ___________ program involves running mapping tasks on many or all of the nodes in our cluster.",
        "Options": [
            "a) MapReduce",
            "b) Map",
            "c) Reducer",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 42,
        "Question": "Mapper implementations are passed the JobConf for the job via the ________ method.",
        "Options": [
            "a) JobConfigure.configure",
            "b) JobConfigurable.configure",
            "c) JobConfigurable.configurable",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 43,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Applications can use the Reporter to report progress",
            "b) The Hadoop MapReduce framework spawns one map task for each InputSplit generated by the InputFormat for the job",
            "c) The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 44,
        "Question": "Input to the _______ is the sorted output of the mappers.",
        "Options": [
            "a) Reducer",
            "b) Mapper",
            "c) Shuffle",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 45,
        "Question": "The right number of reduces seems to be ____________",
        "Options": [
            "a) 0.90",
            "b) 0.80",
            "c) 0.36",
            "d) 0.95",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 46,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Reducer has 2 primary phases",
            "b) Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures",
            "c) It is legal to set the number of reduce-tasks to zero if no reduction is desired",
            "d) The framework groups Reducer inputs by keys (since different mappers may have output the same key) in the sort stage",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 47,
        "Question": "The output of the _______ is not sorted in the Mapreduce framework for Hadoop.",
        "Options": [
            "a) Mapper",
            "b) Cascader",
            "c) Scalding",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 48,
        "Question": "Which of the following phases occur simultaneously?",
        "Options": [
            "a) Shuffle and Sort",
            "b) Reduce and Sort",
            "c) Shuffle and Map",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 49,
        "Question": "Mapper and Reducer implementations can use the ________ to report progress or just indicate that they are alive.",
        "Options": [
            "a) Partitioner",
            "b) OutputCollector",
            "c) Reporter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 50,
        "Question": "__________ is a generalization of the facility provided by the MapReduce framework to collect data output by the Mapper or the Reducer.",
        "Options": [
            "a) Partitioner",
            "b) OutputCollector",
            "c) Reporter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 51,
        "Question": "_________ is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution.",
        "Options": [
            "a) Map Parameters",
            "b) JobConf",
            "c) MemoryConf",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 52,
        "Question": "________ systems are scale-out file-based (HDD) systems moving to more uses of memory in the nodes.",
        "Options": [
            "a) NoSQL",
            "b) NewSQL",
            "c) SQL",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 53,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop is ideal for the analytical, post-operational, data-warehouse-ish type of workload",
            "b) HDFS runs on a small cluster of commodity-class nodes",
            "c) NEWSQL is frequently the collection point for big data",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 54,
        "Question": "Hadoop data is not sequenced and is in 64MB to 256MB block sizes of delimited record values with schema applied on read based on ____________",
        "Options": [
            "a) HCatalog",
            "b) Hive",
            "c) Hbase",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 55,
        "Question": "__________ are highly resilient and eliminate the single-point-of-failure risk with traditional Hadoop deployments.",
        "Options": [
            "a) EMR",
            "b) Isilon solutions",
            "c) AWS",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 56,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) EMC Isilon Scale-out Storage Solutions for Hadoop combine a powerful yet simple and highly efficient storage platform",
            "b) Isilon native HDFS integration means you can avoid the need to invest in a separate Hadoop infrastructure",
            "c) NoSQL systems do provide high latency access and accommodate less concurrent users",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 57,
        "Question": "HDFS and NoSQL file systems focus almost exclusively on adding nodes to ____________",
        "Options": [
            "a) Scale out",
            "b) Scale up",
            "c) Both Scale out and up",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 58,
        "Question": "Which is the most popular NoSQL database for scalable big data store with Hadoop?",
        "Options": [
            "a) Hbase",
            "b) MongoDB",
            "c) Cassandra",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 59,
        "Question": "The ___________ can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks.",
        "Options": [
            "a) DataCache",
            "b) DistributedData",
            "c) DistributedCache",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 60,
        "Question": "HBase provides ___________ like capabilities on top of Hadoop and HDFS.",
        "Options": [
            "a) TopTable",
            "b) BigTop",
            "c) Bigtable",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 61,
        "Question": "__________ refers to incremental costs with no major impact on solution design, performance and complexity.",
        "Options": [
            "a) Scale-out",
            "b) Scale-down",
            "c) Scale-up",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 62,
        "Question": "Streaming supports streaming command options as well as _________ command options.",
        "Options": [
            "a) generic",
            "b) tool",
            "c) library",
            "d) task",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 63,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can specify any executable as the mapper and/or the reducer",
            "b) You cannot supply a Java class as the mapper and/or the reducer",
            "c) The class you supply for the output format should return key/value pairs of Text class",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 64,
        "Question": "Which of the following Hadoop streaming command option parameter is required?",
        "Options": [
            "a) output directoryname",
            "b) mapper executable",
            "c) input directoryname",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 65,
        "Question": "To set an environment variable in a streaming command use ____________",
        "Options": [
            "a) -cmden EXAMPLE_DIR=/home/example/dictionaries/",
            "b) -cmdev EXAMPLE_DIR=/home/example/dictionaries/",
            "c) -cmdenv EXAMPLE_DIR=/home/example/dictionaries/",
            "d) -cmenv EXAMPLE_DIR=/home/example/dictionaries/",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 66,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop has a library package called Aggregate",
            "b) Aggregate allows you to define a mapper plugin class that is expected to generate “aggregatable items” for each input key/value pair of the mappers",
            "c) To use Aggregate, simply specify “-mapper aggregate”",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 67,
        "Question": "The ________ option allows you to copy jars locally to the current working directory of tasks and automatically unjar the files.",
        "Options": [
            "a) archives",
            "b) files",
            "c) task",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 68,
        "Question": "______________ class allows the Map/Reduce framework to partition the map outputs based on certain key fields, not the whole keys.",
        "Options": [
            "a) KeyFieldPartitioner",
            "b) KeyFieldBasedPartitioner",
            "c) KeyFieldBased",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 69,
        "Question": "Which of the following class provides a subset of features provided by the Unix/GNU Sort?",
        "Options": [
            "a) KeyFieldBased",
            "b) KeyFieldComparator",
            "c) KeyFieldBasedComparator",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 70,
        "Question": "Which of the following class is provided by the Aggregate package?",
        "Options": [
            "a) Map",
            "b) Reducer",
            "c) Reduce",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 71,
        "Question": "Hadoop has a library class, org.apache.hadoop.mapred.lib.FieldSelectionMapReduce, that effectively allows you to process text data like the unix ______ utility.",
        "Options": [
            "a) Copy",
            "b) Cut",
            "c) Paste",
            "d) Move",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 72,
        "Question": "A ________ serves as the master and there is only one NameNode per cluster.",
        "Options": [
            "a) Data Node",
            "b) NameNode",
            "c) Data block",
            "d) Replication",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 73,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) DataNode is the slave/worker node and holds the user data in the form of Data Blocks",
            "b) Each incoming file is broken into 32 MB by default",
            "c) Data blocks are replicated across different nodes in the cluster to ensure a low degree of fault tolerance",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 74,
        "Question": "HDFS works in a  __________ fashion.",
        "Options": [
            "a) master-worker",
            "b) master-slave",
            "c) worker/slave",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 75,
        "Question": "________ NameNode is used when the Primary NameNode goes down.",
        "Options": [
            "a) Rack",
            "b) Data",
            "c) Secondary",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 76,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Replication Factor can be configured at a cluster level (Default is set to 3) and also at a file level",
            "b) Block Report from each DataNode contains a list of all the blocks that are stored on that DataNode",
            "c) User data is stored on the local file system of DataNodes",
            "d) DataNode is aware of the files to which the blocks stored on it belong to",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 77,
        "Question": "Which of the following scenario may not be a good fit for HDFS?",
        "Options": [
            "a) HDFS is not suitable for scenarios requiring multiple/simultaneous writes to the same file",
            "b) HDFS is suitable for storing data related to applications requiring low latency data access",
            "c) HDFS is suitable for storing data related to applications requiring low latency data access",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 78,
        "Question": "The need for data replication can arise in various scenarios like ____________",
        "Options": [
            "a) Replication Factor is changed",
            "b) DataNode goes down",
            "c) Data Blocks get corrupted",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 79,
        "Question": "________ is the slave/worker node and holds the user data in the form of Data Blocks.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) Data block",
            "d) Replication",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 80,
        "Question": "HDFS provides a command line interface called __________ used to interact with HDFS.",
        "Options": [
            "a) “HDFS Shell”",
            "b) “FS Shell”",
            "c) “DFS Shell”",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 81,
        "Question": "HDFS is implemented in _____________ programming language.",
        "Options": [
            "a) C++",
            "b) Java",
            "c) Scala",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 82,
        "Question": "For YARN, the ___________ Manager UI provides host and port information.",
        "Options": [
            "a) Data Node",
            "b) NameNode",
            "c) Resource",
            "d) Replication",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 83,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The Hadoop framework publishes the job flow status to an internally running web server on the master nodes of the Hadoop cluster",
            "b) Each incoming file is broken into 32 MB by default",
            "c) Data blocks are replicated across different nodes in the cluster to ensure a low degree of fault tolerance",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 84,
        "Question": "For ________ the HBase Master UI provides information about the HBase Master uptime.",
        "Options": [
            "a) HBase",
            "b) Oozie",
            "c) Kafka",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 85,
        "Question": "During start up, the ___________ loads the file system state from the fsimage and the edits log file.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 86,
        "Question": "In order to read any file in HDFS, instance of __________ is required.",
        "Options": [
            "a) filesystem",
            "b) datastream",
            "c) outstream",
            "d) inputstream",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 87,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The framework groups Reducer inputs by keys",
            "b) The shuffle and sort phases occur simultaneously i.e. while outputs are being fetched they are merged",
            "c) Since JobConf.setOutputKeyComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 88,
        "Question": "______________ is method to copy byte from input stream to any other stream in Hadoop.",
        "Options": [
            "a) IOUtils",
            "b) Utils",
            "c) IUtils",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 89,
        "Question": "_____________ is used to read data from bytes buffers.",
        "Options": [
            "a) write()",
            "b) read()",
            "c) readwrite()",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 90,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The framework calls reduce method for each <key, (list of values)> pair in the grouped inputs",
            "b) The output of the Reducer is re-sorted",
            "c) reduce method reduces values for a given key",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 91,
        "Question": "Interface  ____________ reduces a set of intermediate values which share a key to a smaller set of values.",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Writable",
            "d) Readable",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 92,
        "Question": "Reducer is input the grouped output of a ____________",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Writable",
            "d) Readable",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 93,
        "Question": "The output of the reduce task is typically written to the FileSystem via ____________",
        "Options": [
            "a) OutputCollector",
            "b) InputCollector",
            "c) OutputCollect",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 94,
        "Question": "Applications can use the _________ provided to report progress or just indicate that they are alive.",
        "Options": [
            "a) Collector",
            "b) Reporter",
            "c) Dashboard",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 95,
        "Question": "Which of the following parameter is to collect keys and combined values?",
        "Options": [
            "a) key",
            "b) values",
            "c) reporter",
            "d) output",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 96,
        "Question": "________ is a programming model designed for processing large volumes of data in parallel by dividing the work into a set of independent tasks.",
        "Options": [
            "a) Hive",
            "b) MapReduce",
            "c) Pig",
            "d) Lucene",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 97,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Data locality means movement of the algorithm to the data instead of data to algorithm",
            "b) When the processing is done on the data algorithm is moved across the Action Nodes rather than data to the algorithm",
            "c) Moving Computation is expensive than Moving Data",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 98,
        "Question": "The daemons associated with the MapReduce phase are ________ and task-trackers.",
        "Options": [
            "a) job-tracker",
            "b) map-tracker",
            "c) reduce-tracker",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 99,
        "Question": "The JobTracker pushes work out to available _______ nodes in the cluster, striving to keep the work as close to the data as possible.",
        "Options": [
            "a) DataNodes",
            "b) TaskTracker",
            "c) ActionNodes",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 100,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The map function in Hadoop MapReduce have the following general form:map:(K1, V1) -> list(K2, V2)",
            "b) The reduce function in Hadoop MapReduce have the following general form: reduce: (K2, list(V2)) -> list(K3, V3)",
            "c) MapReduce has a complex model of data processing: inputs and outputs for the map and reduce functions are key-value pairs",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 101,
        "Question": "InputFormat class calls the ________ function and computes splits for each file and then sends them to the jobtracker.",
        "Options": [
            "a) puts",
            "b) gets",
            "c) getSplits",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 102,
        "Question": " On a tasktracker, the map task passes the split to the createRecordReader() method on InputFormat to obtain a _________ for that split.",
        "Options": [
            "a) InputReader",
            "b) RecordReader",
            "c) OutputReader",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 103,
        "Question": "The default InputFormat is __________ which treats each value of input a new value and the associated key is byte offset.",
        "Options": [
            "a) TextFormat",
            "b) TextInputFormat",
            "c) InputFormat",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 104,
        "Question": "__________ controls the partitioning of the keys of the intermediate map-outputs.",
        "Options": [
            "a) Collector",
            "b) Partitioner",
            "c) InputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 105,
        "Question": "Output of the mapper is first written on the local disk for sorting and _________ process.",
        "Options": [
            "a) shuffling",
            "b) secondary sorting",
            "c) forking",
            "d) reducing",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 106,
        "Question": "_________ is the name of the archive you would like to create.",
        "Options": [
            "a) archive",
            "b) archiveName",
            "c) name",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 107,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) A Hadoop archive maps to a file system directory",
            "b) Hadoop archives are special format archives",
            "c) A Hadoop archive always has a *.har extension",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 108,
        "Question": "Using Hadoop Archives in __________ is as easy as specifying a different input filesystem than the default file system.",
        "Options": [
            "a) Hive",
            "b) Pig",
            "c) MapReduce",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 109,
        "Question": "The __________ guarantees that excess resources taken from a queue will be restored to it within N minutes of its need for them.",
        "Options": [
            "a) capacitor",
            "b) scheduler",
            "c) datanode",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 110,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Hadoop archive exposes itself as a file system layer",
            "b) Hadoop archives are immutable",
            "c) Archive rename, deletes and creates return an error",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 111,
        "Question": "_________ is a pluggable Map/Reduce scheduler for Hadoop which provides a way to share large clusters.",
        "Options": [
            "a) Flow Scheduler",
            "b) Data Scheduler",
            "c) Capacity Scheduler",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 112,
        "Question": "Which of the following parameter describes destination directory which would contain the archive?",
        "Options": [
            "a) -archiveName <name>",
            "b) <source>",
            "c) <destination>",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 113,
        "Question": "_________ identifies filesystem path names which work as usual with regular expressions.",
        "Options": [
            "a) -archiveName <name>",
            "b) <source>",
            "c) <destination>",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 114,
        "Question": " __________ is the parent argument used to specify the relative path to which the files should be archived to",
        "Options": [
            "a) -archiveName <name>",
            "b) -p <parent_path>",
            "c) <destination>",
            "d) <source>",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 115,
        "Question": "Hadoop I/O Hadoop comes with a set of ________ for data I/O.",
        "Options": [
            "a) methods",
            "b) commands",
            "c) classes",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 116,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The sequence file also can contain a “secondary” key-value list that can be used as file Metadata",
            "b) SequenceFile formats share a header that contains some information which allows the reader to recognize is format",
            "c) There’re Key and Value Class Name’s that allow the reader to instantiate those classes, via reflection, for reading",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 117,
        "Question": "Apache Hadoop ___________ provides a persistent data structure for binary key-value pairs.",
        "Options": [
            "a) GetFile",
            "b) SequenceFile",
            "c) Putfile",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 118,
        "Question": "How many formats of SequenceFile are present in Hadoop I/O?",
        "Options": [
            "a) 2",
            "b) 3",
            "c) 4",
            "d) 5",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 119,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The data file contains all the key, value records but key N + 1 must be greater than or equal to the key N",
            "b) Sequence file is a kind of hadoop file based data structure",
            "c) Map file type is splittable as it contains a sync point after several records",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 120,
        "Question": "Which of the following format is more compression-aggressive?",
        "Options": [
            "a) Partition Compressed",
            "b) Record Compressed",
            "c) Block-Compressed",
            "d) Uncompressed",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 121,
        "Question": "The __________ is a directory that contains two SequenceFile.",
        "Options": [
            "a) ReduceFile",
            "b) MapperFile",
            "c) MapFile",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 122,
        "Question": "The ______ file is populated with the key and a LongWritable that contains the starting byte position of the record.",
        "Options": [
            "a) Array",
            "b) Index",
            "c) Immutable",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 123,
        "Question": "The _________ as just the value field append(value) and the key is a LongWritable that contains the record number, count + 1.",
        "Options": [
            "a) SetFile",
            "b) ArrayFile",
            "c) BloomMapFile",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 124,
        "Question": "____________  data file takes is based on avro serialization framework which was primarily created for hadoop.",
        "Options": [
            "a) Oozie",
            "b) Avro",
            "c) cTakes",
            "d) Lucene",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 125,
        "Question": "The _________ codec from Google provides modest compression ratios.",
        "Options": [
            "a) Snapcheck",
            "b) Snappy",
            "c) FileCompress",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 126,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Snappy is licensed under the GNU Public License (GPL)",
            "b) BgCIK needs to create an index when it compresses a file",
            "c) The Snappy codec is integrated into Hadoop Common, a set of common utilities that supports other Hadoop subprojects",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 127,
        "Question": "Which of the following compression is similar to Snappy compression?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 128,
        "Question": "Which of the following supports splittable compression?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 129,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) From a usability standpoint, LZO and Gzip are similar",
            "b) Bzip2 generates a better compression ratio than does Gzip, but it’s much slower",
            "c) Gzip is a compression utility that was adopted by the GNU project",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 130,
        "Question": "Which of the following is the slowest compression technique?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 131,
        "Question": "Gzip (short for GNU zip) generates compressed files that have a _________ extension.",
        "Options": [
            "a) .gzip",
            "b) .gz",
            "c) .gzp",
            "d) .g",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 132,
        "Question": "Which of the following is based on the DEFLATE algorithm?",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 133,
        "Question": "__________  typically compresses files to within 10% to 15% of the best available techniques.",
        "Options": [
            "a) LZO",
            "b) Bzip2",
            "c) Gzip",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 134,
        "Question": "The LZO compression format is composed of approximately __________ blocks of compressed data.",
        "Options": [
            "a) 128k",
            "b) 256k",
            "c) 24k",
            "d) 36k",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 135,
        "Question": "The HDFS client software implements __________ checking on the contents of HDFS files.",
        "Options": [
            "a) metastore",
            "b) parity",
            "c) checksum",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 136,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The HDFS architecture is compatible with data rebalancing schemes",
            "b) Datablocks support storing a copy of data at a particular instant of time",
            "c) HDFS currently support snapshots",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 137,
        "Question": "The ___________ machine is a single point of failure for an HDFS cluster.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 138,
        "Question": "The ____________ and the EditLog are central data structures of HDFS.",
        "Options": [
            "a) DsImage",
            "b) FsImage",
            "c) FsImages",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 139,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HDFS is designed to support small files only",
            "b) Any update to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously",
            "c) NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 140,
        "Question": "__________ support storing a copy of data at a particular instant of time.",
        "Options": [
            "a) Data Image",
            "b) Datanots",
            "c) Snapshots",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 141,
        "Question": "Automatic restart and ____________ of the NameNode software to another machine is not supported.",
        "Options": [
            "a) failover",
            "b) end",
            "c) scalability",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 142,
        "Question": "HDFS, by default, replicates each data block _____ times on different nodes and on at least ____ racks.",
        "Options": [
            "a) 3, 2",
            "b) 1, 2",
            "c) 2, 3",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 143,
        "Question": "_________ stores its metadata on multiple disks that typically include a non-local file server.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 144,
        "Question": "The HDFS file system is temporarily unavailable whenever the HDFS ________ is down.",
        "Options": [
            "a) DataNode",
            "b) NameNode",
            "c) ActionNode",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 145,
        "Question": "Apache _______ is a serialization framework that produces data in a compact binary format.",
        "Options": [
            "a) Oozie",
            "b) Impala",
            "c) kafka",
            "d) Avro",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 146,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Apache Avro is a framework that allows you to serialize data in a format that has a schema built in",
            "b) The serialized data is in a compact binary format that doesn’t require proxy objects or code generation",
            "c) Including schemas with the Avro messages allows any application to deserialize the data",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 147,
        "Question": "Avro schemas describe the format of the message and are defined using ______________",
        "Options": [
            "a) JSON",
            "b) XML",
            "c) JS",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 148,
        "Question": "The ____________ is an iterator which reads through the file and returns objects using the next() method.",
        "Options": [
            "a) DatReader",
            "b) DatumReader",
            "c) DatumRead",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 149,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Java code is used to deserialize the contents of the file into objects",
            "b) Avro allows you to use complex data structures within Hadoop MapReduce jobs",
            "c) The m2e plugin automatically downloads the newly added JAR files and their dependencies",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 150,
        "Question": "The ____________ class extends and implements several Hadoop-supplied interfaces.",
        "Options": [
            "a) AvroReducer",
            "b) Mapper",
            "c) AvroMapper",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 151,
        "Question": "____________ class accepts the values that the ModelCountMapper object has collected.",
        "Options": [
            "a) AvroReducer",
            "b) Mapper",
            "c) AvroMapper",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 152,
        "Question": "The ________ method in the ModelCountReducer class “reduces” the values the mapper collects into a derived value.",
        "Options": [
            "a) count",
            "b) add",
            "c) reduce",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 153,
        "Question": "Which of the following works well with Avro?",
        "Options": [
            "a) Lucene",
            "b) kafka",
            "c) MapReduce",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 154,
        "Question": "__________ tools is used to generate proxy objects in Java to easily work with the objects.",
        "Options": [
            "a) Lucene",
            "b) kafka",
            "c) MapReduce",
            "d) Avro",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 155,
        "Question": "Avro schemas are defined with _____",
        "Options": [
            "a) JSON",
            "b) XML",
            "c) JAVA",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 156,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Avro provides functionality similar to systems such as Thrift",
            "b) When Avro is used in RPC, the client and server exchange data in the connection handshake",
            "c) Apache Avro, Avro, Apache, and the Avro and Apache logos are trademarks of The Java Foundation",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 157,
        "Question": "__________ facilitates construction of generic data-processing systems and languages.",
        "Options": [
            "a) Untagged data",
            "b) Dynamic typing",
            "c) No manually-assigned field IDs",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 158,
        "Question": "With ______ we can store data and read it easily with various programming languages.",
        "Options": [
            "a) Thrift",
            "b) Protocol Buffers",
            "c) Avro",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 159,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Apache Avro™ is a data serialization system",
            "b) Avro provides simple integration with dynamic languages",
            "c) Avro provides rich data structures",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 160,
        "Question": "________ are a way of encoding structured data in an efficient yet extensible format.",
        "Options": [
            "a) Thrift",
            "b) Protocol Buffers",
            "c) Avro",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 161,
        "Question": "Thrift resolves possible conflicts through _________ of the field.",
        "Options": [
            "a) Name",
            "b) Static number",
            "c) UID",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 162,
        "Question": "Avro is said to be the future _______ layer of Hadoop.",
        "Options": [
            "a) RMC",
            "b) RPC",
            "c) RDC",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 163,
        "Question": "When using reflection to automatically build our schemas without code generation, we need to configure Avro using?",
        "Options": [
            "a) AvroJob.Reflect(jConf);",
            "b) AvroJob.setReflect(jConf);",
            "c) Job.setReflect(jConf);",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 164,
        "Question": "We can declare the schema of our data either in a ______ file.",
        "Options": [
            "a) JSON",
            "b) XML",
            "c) SQL",
            "d) R",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 165,
        "Question": "Which of the following is a primitive data type in Avro?",
        "Options": [
            "a) null",
            "b) boolean",
            "c) float",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 166,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Records use the type name “record” and support three attributes",
            "b) Enum are represented using JSON arrays",
            "c) Avro data is always serialized with its schema",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 167,
        "Question": "Avro supports ______ kinds of complex types.",
        "Options": [
            "a) 3",
            "b) 4",
            "c) 6",
            "d) 7",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 168,
        "Question": "________ are encoded as a series of blocks.",
        "Options": [
            "a) Arrays",
            "b) Enum",
            "c) Unions",
            "d) Maps",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 169,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Record, enums and fixed are named types",
            "b) Unions may immediately contain other unions",
            "c) A namespace is a dot-separated sequence of such names",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 170,
        "Question": "________ instances are encoded using the number of bytes declared in the schema.",
        "Options": [
            "a) Fixed",
            "b) Enum",
            "c) Unions",
            "d) Maps",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 171,
        "Question": "________ permits data written by one system to be efficiently sorted by another system.",
        "Options": [
            "a) Complex Data type",
            "b) Order",
            "c) Sort Order",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 172,
        "Question": "_____________ are used between blocks to permit efficient splitting of files for MapReduce processing.",
        "Options": [
            "a) Codec",
            "b) Data Marker",
            "c) Synchronization markers",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 173,
        "Question": "The __________ codec uses Google’s Snappy compression library.",
        "Options": [
            "a) null",
            "b) snappy",
            "c) deflate",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 174,
        "Question": "Avro messages are framed as a list of _________",
        "Options": [
            "a) buffers",
            "b) frames",
            "c) rows",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 175,
        "Question": "_______ can change the maximum number of cells of a column family.",
        "Options": [
            "a) set",
            "b) reset",
            "c) alter",
            "d) select",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 176,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can add a column family to a table using the method addColumn()",
            "b) Using alter, you can also create a column family",
            "c) Using disable-all, you can truncate a column family",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 177,
        "Question": "Which of the following is not a table scope operator?",
        "Options": [
            "a) MEMSTORE_FLUSH",
            "b) MEMSTORE_FLUSHSIZE",
            "c) MAX_FILESIZE",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 178,
        "Question": "You can delete a column family from a table using the method _________ of HBAseAdmin class.",
        "Options": [
            "a) delColumn()",
            "b) removeColumn()",
            "c) deleteColumn()",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 179,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To read data from an HBase table, use the get() method of the HTable class",
            "b) You can retrieve data from the HBase table using the get() method of the HTable class",
            "c) While retrieving data, you can get a single row by id, or get a set of rows by a set of row ids, or scan an entire table or a subset of rows",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 180,
        "Question": "__________ class adds HBase configuration files to its object.",
        "Options": [
            "a) Configuration",
            "b) Collector",
            "c) Component",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 181,
        "Question": "The ________ class provides the getValue() method to read the values from its instance.",
        "Options": [
            "a) Get",
            "b) Result",
            "c) Put",
            "d) Value",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 182,
        "Question": "________ communicate with the client and handle data-related operations.",
        "Options": [
            "a) Master Server",
            "b) Region Server",
            "c) Htable",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 183,
        "Question": "_________ is the main configuration file of HBase.",
        "Options": [
            "a) hbase.xml",
            "b) hbase-site.xml",
            "c) hbase-site-conf.xml",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 184,
        "Question": "HBase uses the _______ File System to store its data.",
        "Options": [
            "a) Hive",
            "b) Imphala",
            "c) Hadoop",
            "d) Scala",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 185,
        "Question": "The Mapper implementation processes one line at a time via _________ method.",
        "Options": [
            "a) map",
            "b) reduce",
            "c) mapper",
            "d) reducer",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 186,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Mapper maps input key/value pairs to a set of intermediate key/value pairs",
            "b) Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods",
            "c) Mapper and Reducer interfaces form the core of the job",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 187,
        "Question": "The Hadoop MapReduce framework spawns one map task for each __________ generated by the InputFormat for the job.",
        "Options": [
            "a) OutputSplit",
            "b) InputSplit",
            "c) InputSplitStream",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 188,
        "Question": "Users can control which keys (and hence records) go to which Reducer by implementing a custom?",
        "Options": [
            "a) Partitioner",
            "b) OutputSplit",
            "c) Reporter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 189,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Mapper outputs are sorted and then partitioned per Reducer",
            "b) The total number of partitions is the same as the number of reduce tasks for the job",
            "c) The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 190,
        "Question": "Applications can use the ____________ to report progress and set application-level status messages.",
        "Options": [
            "a) Partitioner",
            "b) OutputSplit",
            "c) Reporter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 191,
        "Question": "The right level of parallelism for maps seems to be around _________ maps per-node.",
        "Options": [
            "a) 1-10",
            "b) 10-100",
            "c) 100-150",
            "d) 150-200",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 192,
        "Question": "The number of reduces for the job is set by the user via _________",
        "Options": [
            "a) JobConf.setNumTasks(int)",
            "b) JobConf.setNumReduceTasks(int)",
            "c) JobConf.setNumMapTasks(int)",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 193,
        "Question": "The framework groups Reducer inputs by key in _________ stage.",
        "Options": [
            "a) sort",
            "b) shuffle",
            "c) reduce",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 194,
        "Question": "The output of the reduce task is typically written to the FileSystem via _____________",
        "Options": [
            "a) OutputCollector.collect",
            "b) OutputCollector.get",
            "c) OutputCollector.receive",
            "d) OutputCollector.put",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 195,
        "Question": "Which of the following is the default Partitioner for Mapreduce?",
        "Options": [
            "a) MergePartitioner",
            "b) HashedPartitioner",
            "c) HashPartitioner",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 196,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The right number of reduces seems to be 0.95 or 1.75",
            "b) Increasing the number of reduces increases the framework overhead",
            "c) With 0.95 all of the reduces can launch immediately and start transferring map outputs as the maps finish",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 197,
        "Question": "Which of the following partitions the key space?",
        "Options": [
            "a) Partitioner",
            "b) Compactor",
            "c) Collector",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 198,
        "Question": "____________ is a generalization of the facility provided by the MapReduce framework to collect data output by the Mapper or the Reducer.",
        "Options": [
            "a) OutputCompactor",
            "b) OutputCollector",
            "c) InputCollector",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 199,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) It is legal to set the number of reduce-tasks to zero if no reduction is desired",
            "b) The outputs of the map-tasks go directly to the FileSystem",
            "c) The Mapreduce framework does not sort the map-outputs before writing them out to the FileSystem",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 200,
        "Question": "__________ is the primary interface for a user to describe a MapReduce job to the Hadoop framework for execution.",
        "Options": [
            "a) JobConfig",
            "b) JobConf",
            "c) JobConfiguration",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 201,
        "Question": "The ___________ executes the Mapper/ Reducer task as a child process in a separate jvm.",
        "Options": [
            "a) JobTracker",
            "b) TaskTracker",
            "c) TaskScheduler",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 202,
        "Question": "Maximum virtual memory of the launched child-task is specified using _________",
        "Options": [
            "a) mapv",
            "b) mapred",
            "c) mapvim",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 203,
        "Question": "Which of the following parameter is the threshold for the accounting and serialization buffers?",
        "Options": [
            "a) io.sort.spill.percent",
            "b) io.sort.record.percent",
            "c) io.sort.mb",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 204,
        "Question": "______________ is percentage of memory relative to the maximum heap size in which map outputs may be retained during the reduce.",
        "Options": [
            "a) mapred.job.shuffle.merge.percent",
            "b) mapred.job.reduce.input.buffer.percen",
            "c) mapred.inmem.merge.threshold",
            "d) io.sort.factor",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 205,
        "Question": "____________ specifies the number of segments on disk to be merged at the same time.",
        "Options": [
            "a) mapred.job.shuffle.merge.percent",
            "b) mapred.job.reduce.input.buffer.percen",
            "c) mapred.inmem.merge.threshold",
            "d) io.sort.factor",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 206,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The number of sorted map outputs fetched into memory before being merged to disk",
            "b) The memory threshold for fetched map outputs before an in-memory merge is finished",
            "c) The percentage of memory relative to the maximum heap size in which map outputs may not be retained during the reduce",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 207,
        "Question": "Map output larger than ___________ percent of the memory allocated to copying map outputs.",
        "Options": [
            "a) 10",
            "b) 15",
            "c) 25",
            "d) 35",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 208,
        "Question": "Jobs can enable task JVMs to be reused by specifying the job configuration _________",
        "Options": [
            "a) mapred.job.recycle.jvm.num.tasks",
            "b) mapissue.job.reuse.jvm.num.tasks",
            "c) mapred.job.reuse.jvm.num.tasks",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 209,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The task tracker has local directory to create localized cache and localized job",
            "b) The task tracker can define multiple local directories",
            "c) The Job tracker cannot define multiple local directories",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 210,
        "Question": "During the execution of a streaming job, the names of the _______ parameters are transformed.",
        "Options": [
            "a) vmap",
            "b) mapvim",
            "c) mapreduce",
            "d) mapred",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 211,
        "Question": "The standard output (stdout) and error (stderr) streams of the task are read by the TaskTracker and logged to _________",
        "Options": [
            "a) ${HADOOP_LOG_DIR}/user",
            "b) ${HADOOP_LOG_DIR}/userlogs",
            "c) ${HADOOP_LOG_DIR}/logs",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 212,
        "Question": "____________ is the primary interface by which user-job interacts with the JobTracker.",
        "Options": [
            "a) JobConf",
            "b) JobClient",
            "c) JobServer",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 213,
        "Question": "The _____________ can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks.",
        "Options": [
            "a) DistributedLog",
            "b) DistributedCache",
            "c) DistributedJars",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 214,
        "Question": "__________ is used to filter log files from the output directory listing.",
        "Options": [
            "a) OutputLog",
            "b) OutputLogFilter",
            "c) DistributedLog",
            "d) DistributedJars",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 215,
        "Question": "Which of the following class provides access to configuration parameters?",
        "Options": [
            "a) Config",
            "b) Configuration",
            "c) OutputConfig",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 216,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Configuration parameters may be declared static",
            "b) Unless explicitly turned off, Hadoop by default specifies two resources",
            "c) Configuration class provides access to configuration parameters",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 217,
        "Question": "___________ gives site-specific configuration for a given hadoop installation.",
        "Options": [
            "a) core-default.xml",
            "b) core-site.xml",
            "c) coredefault.xml",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 218,
        "Question": "Administrators typically define parameters as final in __________ for values that user applications may not alter.",
        "Options": [
            "a) core-default.xml",
            "b) core-site.xml",
            "c) coredefault.xml",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 219,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) addDeprecations adds a set of deprecated keys to the global deprecations",
            "b) configuration parameters cannot be declared final",
            "c) addDeprecations method is lockless",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 220,
        "Question": "_________ method clears all keys from the configuration.",
        "Options": [
            "a) clear",
            "b) addResource",
            "c) getClass",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 221,
        "Question": "________ method adds the deprecated key to the global deprecation map.",
        "Options": [
            "a) addDeprecits",
            "b) addDeprecation",
            "c) keyDeprecation",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 222,
        "Question": "________ checks whether the given key is deprecated.",
        "Options": [
            "a) isDeprecated",
            "b) setDeprecated",
            "c) isDeprecatedif",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 223,
        "Question": "_________ is useful for iterating the properties when all deprecated properties for currently set properties need to be present.",
        "Options": [
            "a) addResource",
            "b) setDeprecatedProperties",
            "c) addDefaultResource",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 224,
        "Question": "Which of the following adds a configuration resource?",
        "Options": [
            "a) addResource",
            "b) setDeprecatedProperties",
            "c) addDefaultResource",
            "d) addResource",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 225,
        "Question": "For running hadoop service daemons in Hadoop in secure mode ___________ principals are required.",
        "Options": [
            "a) SSL",
            "b) Kerberos",
            "c) SSH",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 226,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop does have the definition of group by itself",
            "b) MapReduce JobHistory server run as same user such as mapred",
            "c) SSO environment is managed using Kerberos with LDAP for Hadoop in secure mode",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 227,
        "Question": "The simplest way to do authentication is using _________ command of Kerberos.",
        "Options": [
            "a) auth",
            "b) kinit",
            "c) authorize",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 228,
        "Question": "Data transfer between Web-console and clients are protected by using _________",
        "Options": [
            "a) SSL",
            "b) Kerberos",
            "c) SSH",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 229,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Data transfer protocol of DataNode does not use the RPC framework of Hadoop",
            "b) Apache Oozie which access the services of Hadoop on behalf of end users need to be able to impersonate end users",
            "c) DataNode must authenticate itself by using privileged ports which are specified by dfs.datanode.address and dfs.datanode.http.address",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 230,
        "Question": "In order to turn on RPC authentication in hadoop, set the value of hadoop.security.authentication property to _________",
        "Options": [
            "a) zero",
            "b) kerberos",
            "c) false",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 231,
        "Question": "The __________ provides a proxy between the web applications exported by an application and an end user.",
        "Options": [
            "a) ProxyServer",
            "b) WebAppProxy",
            "c) WebProxy",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 232,
        "Question": "___________ used by YARN framework which defines how any container launched and controlled.",
        "Options": [
            "a) Container",
            "b) ContainerExecutor",
            "c) Executor",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 233,
        "Question": "The ____________ requires that paths including and leading up to the directories specified in yarn.nodemanager.local-dirs.",
        "Options": [
            "a) TaskController",
            "b) LinuxTaskController",
            "c) LinuxController",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 234,
        "Question": "The configuration file must be owned by the user running _________",
        "Options": [
            "a) DataManager",
            "b) NodeManager",
            "c) ValidationManager",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 235,
        "Question": "__________ storage is a solution to decouple growing storage capacity from compute capacity.",
        "Options": [
            "a) DataNode",
            "b) Archival",
            "c) Policy",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 236,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) When there is enough space, block replicas are stored according to the storage type list",
            "b) One_SSD is used for storing all replicas in SSD",
            "c) Hot policy is useful only for single replica blocks",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 237,
        "Question": "___________ is added for supporting writing single replica files in memory.",
        "Options": [
            "a) ROM_DISK",
            "b) ARCHIVE",
            "c) RAM_DISK",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 238,
        "Question": "Which of the following has high storage density?",
        "Options": [
            "a) ROM_DISK",
            "b) ARCHIVE",
            "c) RAM_DISK",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 239,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) A Storage policy consists of the Policy ID",
            "b) The storage policy can be specified using the “dfsadmin -setStoragePolicy” command",
            "c) dfs.storage.policy.enabled is used for enabling/disabling the storage policy feature",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 240,
        "Question": "Which of the following storage policy is used for both storage and compute?",
        "Options": [
            "a) Hot",
            "b) Cold",
            "c) Warm",
            "d) All_SSD",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 241,
        "Question": "Which of the following is only for storage with limited compute?",
        "Options": [
            "a) Hot",
            "b) Cold",
            "c) Warm",
            "d) All_SSD",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 242,
        "Question": "When a block is warm, some of its replicas are stored in DISK and the remaining replicas are stored in _________",
        "Options": [
            "a) ROM_DISK",
            "b) ARCHIVE",
            "c) RAM_DISK",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 243,
        "Question": "____________ is used for storing one of the replicas in SSD.",
        "Options": [
            "a) Hot",
            "b) Lazy_Persist",
            "c) One_SSD",
            "d) All_SSD",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 244,
        "Question": "___________ is used for writing blocks with single replica in memory.",
        "Options": [
            "a) Hot",
            "b) Lazy_Persist",
            "c) One_SSD",
            "d) All_SSD",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 245,
        "Question": "_________ is a  data migration tool added for archiving data.",
        "Options": [
            "a) Mover",
            "b) Hiver",
            "c) Serde",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 246,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Mover is not similar to Balancer",
            "b) hdfs dfsadmin -setStoragePolicy <path> <policyName> puts a storage policy to a file or a directory.",
            "c) addCacheArchive add archives to be localized",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 247,
        "Question": "Which of the following is used to list out the storage policies?",
        "Options": [
            "a) hdfs storagepolicies",
            "b) hdfs storage",
            "c) hd storagepolicies",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 248,
        "Question": "Which of the following statement can be used to get the storage policy of a file or a directory?",
        "Options": [
            "a) hdfs dfsadmin -getStoragePolicy path",
            "b) hdfs dfsadmin -setStoragePolicy path policyName",
            "c) hdfs dfsadmin -listStoragePolicy path policyName",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 249,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) getInstance() creates a new Job with particular cluster",
            "b) getInstance(Configuration conf) creates a new Job with no particular Cluster and a given Configuration",
            "c) getInstance(JobStatus status, Configuration conf) creates a new Job with no particular Cluster and given Configuration and JobStatus",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 250,
        "Question": "Which of the following method is used to get user-specified job name?",
        "Options": [
            "a) getJobName()",
            "b) getJobState()",
            "c) getPriority()",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 251,
        "Question": "__________ get events indicating completion (success/failure) of component tasks.",
        "Options": [
            "a) getJobName()",
            "b) getJobState()",
            "c) getPriority()",
            "d) getTaskCompletionEvents(int startFrom)",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 252,
        "Question": "_________ gets the diagnostic messages for a given task attempt.",
        "Options": [
            "a) getTaskOutputFilter(Configuration conf)",
            "b) getTaskReports(TaskType type)",
            "c) getTrackingURL()",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 253,
        "Question": "reduceProgress() gets the progress of the job’s reduce-tasks, as a float between _________",
        "Options": [
            "a) 0.0-1.0",
            "b) 1.0-2.0",
            "c) 2.0-3.0",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 254,
        "Question": "The Job makes a copy of the _____________ so that any necessary internal modifications do not reflect on the incoming parameter.",
        "Options": [
            "a) Component",
            "b) Configuration",
            "c) Collector",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 255,
        "Question": "________ is the architectural center of Hadoop that allows multiple data processing engines.",
        "Options": [
            "a) YARN",
            "b) Hive",
            "c) Incubator",
            "d) Chuckwa",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 256,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) YARN also extends the power of Hadoop to incumbent and new technologies found within the data center",
            "b) YARN is the central point of investment for Hortonworks within the Apache community",
            "c) YARN enhances a Hadoop compute cluster in many ways",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 257,
        "Question": "YARN’s dynamic allocation of cluster resources improves utilization over more static _______ rules used in early versions of Hadoop.",
        "Options": [
            "a) Hive",
            "b) MapReduce",
            "c) Imphala",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 258,
        "Question": "The __________ is a framework-specific entity that negotiates resources from the ResourceManager.",
        "Options": [
            "a) NodeManager",
            "b) ResourceManager",
            "c) ApplicationMaster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 259,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) From the system perspective, the ApplicationMaster runs as a normal container",
            "b) The ResourceManager is the per-machine slave, which is responsible for launching the applications’ containers",
            "c) The NodeManager is the per-machine slave, which is responsible for launching the applications’ containers, monitoring their resource usage",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 260,
        "Question": "Apache Hadoop YARN stands for _________",
        "Options": [
            "a) Yet Another Reserve Negotiator",
            "b) Yet Another Resource Network",
            "c) Yet Another Resource Negotiator",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 261,
        "Question": "MapReduce has undergone a complete overhaul in hadoop is _________",
        "Options": [
            "a) 0.21",
            "b) 0.23",
            "c) 0.24",
            "d) 0.26",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 262,
        "Question": "The ____________ is the ultimate authority that arbitrates resources among all the applications in the system.",
        "Options": [
            "a) NodeManager",
            "b) ResourceManager",
            "c) ApplicationMaster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 263,
        "Question": "The __________ is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc.",
        "Options": [
            "a) Manager",
            "b) Master",
            "c) Scheduler",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 264,
        "Question": "The CapacityScheduler supports _____________ queues to allow for more predictable sharing of cluster resources.",
        "Options": [
            "a) Networked",
            "b) Hierarchical",
            "c) Partition",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 265,
        "Question": "Yarn commands are invoked by the ________ script.",
        "Options": [
            "a) hive",
            "b) bin",
            "c) hadoop",
            "d) home",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 266,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Each queue has strict ACLs which controls which users can submit applications to individual queues",
            "b) Hierarchy of queues is supported to ensure resources are shared among the sub-queues of an organization",
            "c) Queues are allocated a fraction of the capacity of the grid in the sense that a certain capacity of resources will be at their disposal",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 267,
        "Question": "The queue definitions and properties such as ________ ACLs can be changed, at runtime.",
        "Options": [
            "a) tolerant",
            "b) capacity",
            "c) speed",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 268,
        "Question": "The CapacityScheduler has a predefined queue called _________",
        "Options": [
            "a) domain",
            "b) root",
            "c) rear",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 269,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The multiple of the queue capacity which can be configured to allow a single user to acquire more resources",
            "b) Changing queue properties and adding new queues is very simple",
            "c) Queues cannot be deleted, only addition of new queues is supported",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 270,
        "Question": "The updated queue configuration should be a valid one i.e. queue-capacity at each level should be equal to _________",
        "Options": [
            "a) 50%",
            "b) 75%",
            "c) 100%",
            "d) 0%",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 271,
        "Question": "Users can bundle their Yarn code in a _________ file and execute it using jar command.",
        "Options": [
            "a) java",
            "b) jar",
            "c) C code",
            "d) xml",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 272,
        "Question": "Which of the following command is used to dump the log container?",
        "Options": [
            "a) logs",
            "b) log",
            "c) dump",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 273,
        "Question": "__________ will clear the RMStateStore and is useful if past applications are no longer needed.",
        "Options": [
            "a) -format-state",
            "b) -form-state-store",
            "c) -format-state-store",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 274,
        "Question": "Which of the following command runs ResourceManager admin client?",
        "Options": [
            "a) proxyserver",
            "b) run",
            "c) admin",
            "d) rmadmin",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 275,
        "Question": "___________ generates keys of type LongWritable and values of type Text.",
        "Options": [
            "a) TextOutputFormat",
            "b) TextInputFormat",
            "c) OutputInputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 276,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The reduce input must have the same types as the map output, although the reduce output types may be different again",
            "b) The map input key and value types (K1 and V1) are different from the map output types",
            "c) The partition function operates on the intermediate key",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 277,
        "Question": "In _____________ the default job is similar, but not identical, to the Java equivalent.",
        "Options": [
            "a) Mapreduce",
            "b) Streaming",
            "c) Orchestration",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 278,
        "Question": "An input _________ is a chunk of the input that is processed by a single map.",
        "Options": [
            "a) textformat",
            "b) split",
            "c) datanode",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 279,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) If V2 and V3 are the same, you only need to use setOutputValueClass()",
            "b) The overall effect of Streaming job is to perform a sort of the input",
            "c) A Streaming application can control the separator that is used when a key-value pair is turned into a series of bytes and sent to the map or reduce process over standard input",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 280,
        "Question": "An ___________ is responsible for creating the input splits, and dividing them into records.",
        "Options": [
            "a) TextOutputFormat",
            "b) TextInputFormat",
            "c) OutputInputFormat",
            "d) InputFormat",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 281,
        "Question": "______________ is another implementation of the MapRunnable interface that runs mappers concurrently in a configurable number of threads.",
        "Options": [
            "a) MultithreadedRunner",
            "b) MultithreadedMap",
            "c) MultithreadedMapRunner",
            "d) SinglethreadedMapRunner",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 282,
        "Question": "Which of the following is the only way of running mappers?",
        "Options": [
            "a) MapReducer",
            "b) MapRunner",
            "c) MapRed",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 283,
        "Question": "_________ is the base class for all implementations of InputFormat that use files as their data source.",
        "Options": [
            "a) FileTextFormat",
            "b) FileInputFormat",
            "c) FileOutputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 284,
        "Question": "Which of the following method add a path or paths to the list of inputs?",
        "Options": [
            "a) setInputPaths()",
            "b) addInputPath()",
            "c) setInput()",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 285,
        "Question": "___________ takes node and rack locality into account when deciding which blocks to place in the same split.",
        "Options": [
            "a) CombineFileOutputFormat",
            "b) CombineFileInputFormat",
            "c) TextFileInputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 286,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) With TextInputFormat and KeyValueTextInputFormat, each mapper receives a variable number of lines of input",
            "b) StreamXmlRecordReader, the page elements can be interpreted as records for processing by a mapper",
            "c) The number depends on the size of the split and the length of the lines.",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 287,
        "Question": "The key, a ____________ is the byte offset within the file of the beginning of the line.",
        "Options": [
            "a) LongReadable",
            "b) LongWritable",
            "c) LongWritable",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 288,
        "Question": "_________ is the output produced by TextOutputFor mat, Hadoop default OutputFormat.",
        "Options": [
            "a) KeyValueTextInputFormat",
            "b) KeyValueTextOutputFormat",
            "c) FileValueTextInputFormat",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 289,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop sequence file format stores sequences of binary key-value pairs",
            "b) SequenceFileAsBinaryInputFormat is a variant of SequenceFileInputFormat that retrieves the sequence file’s keys and values as opaque binary objects",
            "c) SequenceFileAsTextInputFormat is a variant of SequenceFileInputFormat that retrieves the sequence file’s keys and values as opaque binary objects.",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 290,
        "Question": "__________ is a variant of SequenceFileInputFormat that converts the sequence file’s keys and values to Text objects.",
        "Options": [
            "a) SequenceFile",
            "b) SequenceFileAsTextInputFormat",
            "c) SequenceAsTextInputFormat",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 291,
        "Question": "__________ class allows you to specify the InputFormat and Mapper to use on a per-path basis.",
        "Options": [
            "a) MultipleOutputs",
            "b) MultipleInputs",
            "c) SingleInputs",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 292,
        "Question": "___________ is an input format for reading data from a relational database, using JDBC.",
        "Options": [
            "a) DBInput",
            "b) DBInputFormat",
            "c) DBInpFormat",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 293,
        "Question": "Which of the following is the default output format?",
        "Options": [
            "a) TextFormat",
            "b) TextOutput",
            "c) TextOutputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 294,
        "Question": "Which of the following writes MapFiles as output?",
        "Options": [
            "a) DBInpFormat",
            "b) MapFileOutputFormat",
            "c) SequenceFileAsBinaryOutputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 295,
        "Question": "The split size is normally the size of a ________ block, which is appropriate for most applications.",
        "Options": [
            "a) Generic",
            "b) Task",
            "c) Library",
            "d) HDFS",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 296,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The minimum split size is usually 1 byte, although some formats have a lower bound on the split size",
            "b) Applications may impose a minimum split size",
            "c) The maximum split size defaults to the maximum value that can be represented by a Java long type",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 297,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop works better with a small number of large files than a large number of small files",
            "b) CombineFileInputFormat is designed to work well with small files",
            "c) CombineFileInputFormat does not compromise the speed at which it can process the input in a typical MapReduce job",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 298,
        "Question": "Which hdfs command is used to check for various inconsistencies?",
        "Options": [
            "a) fsk",
            "b) fsck",
            "c) fetchdt",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 299,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) All hadoop commands are invoked by the bin/hadoop script",
            "b) Hadoop has an option parsing framework that employs only parsing generic options",
            "c) Archive command creates a hadoop archive",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 300,
        "Question": "HDFS supports the ____________ command to fetch Delegation Token and store it in a file on the local system.",
        "Options": [
            "a) fetdt",
            "b) fetchdt",
            "c) fsk",
            "d) rec",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 301,
        "Question": "In ___________ mode, the NameNode will interactively prompt you at the command line about possible courses of action you can take to recover your data.",
        "Options": [
            "a) full",
            "b) partial",
            "c) recovery",
            "d) commit",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 302,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) classNAME displays the class name needed to get the Hadoop jar",
            "b) Balancer Runs a cluster balancing utility",
            "c) An administrator can simply press Ctrl-C to stop the rebalancing process",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 303,
        "Question": "_________ command is used to copy file or directories recursively.",
        "Options": [
            "a) dtcp",
            "b) distcp",
            "c) dcp",
            "d) distc",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 304,
        "Question": "__________ mode is a Namenode state in which it does not accept changes to the name space.",
        "Options": [
            "a) Recover",
            "b) Safe",
            "c) Rollback",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 305,
        "Question": "__________ command is used to interact and view Job Queue information in HDFS.",
        "Options": [
            "a) queue",
            "b) priority",
            "c) dist",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 306,
        "Question": "Which of the following command runs the HDFS secondary namenode?",
        "Options": [
            "a) secondary namenode",
            "b) secondarynamenode",
            "c) secondary_namenode",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 307,
        "Question": "Which of the following is used for the MapReduce job Tracker node?",
        "Options": [
            "a) mradmin",
            "b) tasktracker",
            "c) jobtracker",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 308,
        "Question": "Which of the following is a common hadoop maintenance issue?",
        "Options": [
            "a) Lack of tools",
            "b) Lack of configuration management",
            "c) Lack of web interface",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 309,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) RAID is turned off by default",
            "b) Hadoop is designed to be a highly redundant distributed system",
            "c) Hadoop has a networked configuration system",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 310,
        "Question": "___________ mode allows you to suppress alerts for a host, service, role, or even the entire cluster.",
        "Options": [
            "a) Safe",
            "b) Maintenance",
            "c) Secure",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 311,
        "Question": "Which of the following is a configuration management system?",
        "Options": [
            "a) Alex",
            "b) Puppet",
            "c) Acem",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 312,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) If you set the HBase service into maintenance mode, then its roles (HBase Master and all Region Servers) are put into effective maintenance mode",
            "b) If you set a host into maintenance mode, then any roles running on that host are put into effective maintenance mode",
            "c) Putting a component into maintenance mode prevent events from being logged",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 313,
        "Question": "Which of the following is a common reason to restart hadoop process?",
        "Options": [
            "a) Upgrade Hadoop",
            "b) React to incidents",
            "c) Remove worker nodes",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 314,
        "Question": "__________ Manager’s Service feature monitors dozens of service health and performance metrics about the services and role instances running on your cluster.",
        "Options": [
            "a) Microsoft",
            "b) Cloudera",
            "c) Amazon",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 315,
        "Question": "Which of the tab shows all the role instances that have been instantiated for this service?",
        "Options": [
            "a) Service",
            "b) Status",
            "c) Instance",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 316,
        "Question": "__________ is a standard Java API for monitoring and managing applications.",
        "Options": [
            "a) JVX",
            "b) JVM",
            "c) JMX",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 317,
        "Question": "NameNode is monitored and upgraded in a __________ transition.",
        "Options": [
            "a) safemode",
            "b) securemode",
            "c) servicemode",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 318,
        "Question": "Pig operates in mainly how many nodes?",
        "Options": [
            "a) Two",
            "b) Three",
            "c) Four",
            "d) Five",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 319,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can run Pig in either mode using the “pig” command",
            "b) You can run Pig in batch mode using the Grunt shell",
            "c) You can run Pig in interactive mode using the FS shell",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 320,
        "Question": "You can run Pig in batch mode using __________",
        "Options": [
            "a) Pig shell command",
            "b) Pig scripts",
            "c) Pig options",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 321,
        "Question": "Pig Latin statements are generally organized in one of the following ways?",
        "Options": [
            "a) A LOAD statement to read data from the file system",
            "b) A series of “transformation” statements to process the data",
            "c) A DUMP statement to view results or a STORE statement to save the results",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 322,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To run Pig in local mode, you need access to a single machine",
            "b) The DISPLAY operator will display the results to your terminal screen",
            "c) To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 323,
        "Question": "Which of the following function is used to read data in PIG?",
        "Options": [
            "a) WRITE",
            "b) READ",
            "c) LOAD",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 324,
        "Question": "You can run Pig in interactive mode using the ______ shell.",
        "Options": [
            "a) Grunt",
            "b) FS",
            "c) HDFS",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 325,
        "Question": "Which of the following is the default mode?",
        "Options": [
            "a) Mapreduce",
            "b) Tez",
            "c) Local",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 326,
        "Question": "Which of the following will run pig in local mode?",
        "Options": [
            "a) $ pig -x local …",
            "b) $ pig -x tez_local …",
            "c) $ pig …",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 327,
        "Question": "10.$ pig -x tez_local … will enable ________ mode in Pig.",
        "Options": [
            "a) Mapreduce",
            "b) Tez",
            "c) Local",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 328,
        "Question": "1._________  operator is used to review the schema of a relation.",
        "Options": [
            "a) DUMP",
            "b) DESCRIBE",
            "c) STORE",
            "d) EXPLAIN",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 329,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) During the testing phase of your implementation, you can use LOAD to display results to your terminal screen",
            "b) You can view outer relations as well as relations defined in a nested FOREACH statement",
            "c) Hadoop properties are interpreted by Pig",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 330,
        "Question": "Which of the following operator is used to view the map reduce execution plans?",
        "Options": [
            "a) DUMP",
            "b) DESCRIBE",
            "c) STORE",
            "d) EXPLAIN",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 331,
        "Question": "___________ operator is used to view the step-by-step execution of a series of statements.",
        "Options": [
            "a) ILLUSTRATE",
            "b) DESCRIBE",
            "c) STORE",
            "d) EXPLAIN",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 332,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) ILLUSTRATE operator is used to review how data is transformed through a sequence of Pig Latin statements",
            "b) ILLUSTRATE is based on an example generator",
            "c) Several new private classes make it harder for external tools such as Oozie to integrate with Pig statistics",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 333,
        "Question": "__________ is a framework for collecting and storing script-level statistics for Pig Latin.",
        "Options": [
            "a) Pig Stats",
            "b) PStatistics",
            "c) Pig Statistics",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 334,
        "Question": "The ________ class mimics the behavior of the Main class but gives users a statistics object back.",
        "Options": [
            "a) PigRun",
            "b) PigRunner",
            "c) RunnerPig",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 335,
        "Question": "___________ is a simple xUnit framework that enables you to easily test your Pig scripts.",
        "Options": [
            "a) PigUnit",
            "b) PigXUnit",
            "c) PigUnitX",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 336,
        "Question": "Which of the following will compile the Pigunit?",
        "Options": [
            "a) $pig_trunk ant pigunit-jar",
            "b) $pig_tr ant pigunit-jar",
            "c) $pig_ ant pigunit-jar",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 337,
        "Question": "PigUnit runs in Pig’s _______ mode by default.",
        "Options": [
            "a) local",
            "b) tez",
            "c) mapreduce",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 338,
        "Question": "__________ abstract class has three main methods for loading data and for most use cases it would suffice to extend it.",
        "Options": [
            "a) Load",
            "b) LoadFunc",
            "c) FuncLoad",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 339,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) LoadMeta has methods to convert byte arrays to specific types",
            "b) The Pig load/store API is aligned with Hadoop InputFormat class only",
            "c) LoadPush has methods to push operations from Pig runtime into loader implementations",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 340,
        "Question": "Which of the following has methods to deal with metadata?",
        "Options": [
            "a) LoadPushDown",
            "b) LoadMetadata",
            "c) LoadCaster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 341,
        "Question": "____________ method will be called by Pig both in the front end and back end to pass a unique signature to the Loader.",
        "Options": [
            "a) relativeToAbsolutePath()",
            "b) setUdfContextSignature()",
            "c) getCacheFiles()",
            "d) getShipFiles()",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 342,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The load/store UDFs control how data goes into Pig and comes out of Pig.",
            "b) LoadCaster has methods to convert byte arrays to specific types.",
            "c) The meaning of getNext() has changed and is called by Pig runtime to get the last tuple in the data",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 343,
        "Question": "___________ return a list of hdfs files to ship to distributed cache.",
        "Options": [
            "a) relativeToAbsolutePath()",
            "b) setUdfContextSignature()",
            "c) getCacheFiles()",
            "d) getShipFiles()",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 344,
        "Question": "The loader should use ______ method to communicate the load information to the underlying InputFormat.",
        "Options": [
            "a) relativeToAbsolutePath()",
            "b) setUdfContextSignature()",
            "c) getCacheFiles()",
            "d) setLocation()",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 345,
        "Question": "____________ method enables the RecordReader associated with the InputFormat provided by the LoadFunc is passed to the LoadFunc.",
        "Options": [
            "a) getNext()",
            "b) relativeToAbsolutePath()",
            "c) prepareToRead()",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 346,
        "Question": "__________  method tells LoadFunc which fields are required in the Pig script.",
        "Options": [
            "a) pushProjection()",
            "b) relativeToAbsolutePath()",
            "c) prepareToRead()",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 347,
        "Question": "A loader implementation should implement __________  if casts (implicit or explicit) from DataByteArray fields to other types need to be supported.",
        "Options": [
            "a) LoadPushDown",
            "b) LoadMetadata",
            "c) LoadCaster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 348,
        "Question": "Which of the following is shortcut for DUMP operator?",
        "Options": [
            "a) \\de alias",
            "b) \\d alias",
            "c) \\q",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 349,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Invoke the Grunt shell using the “enter” command",
            "b) Pig does not support jar files",
            "c) Both the run and exec commands are useful for debugging because you can modify a Pig script in an editor",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 350,
        "Question": "Which of the following command is used to show values to keys used in Pig?",
        "Options": [
            "a) set",
            "b) declare",
            "c) display",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 351,
        "Question": "Use the __________ command to run a Pig script that can interact with the Grunt shell (interactive mode).",
        "Options": [
            "a) fetch",
            "b) declare",
            "c) run",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 352,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) You can run Pig scripts from the command line and from the Grunt shell",
            "b) DECLARE defines a Pig macro",
            "c) Use Pig scripts to place Pig Latin statements and Pig commands in a single file",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 353,
        "Question": "Which of the following command can be used for debugging?",
        "Options": [
            "a) exec",
            "b) execute",
            "c) error",
            "d) throw",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 354,
        "Question": "Which of the following file contains user defined functions (UDFs)?",
        "Options": [
            "a) script2-local.pig",
            "b) pig.jar",
            "c) tutorial.jar",
            "d) excite.log.bz2",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 355,
        "Question": "Which of the following is correct syntax for parameter substitution using cmd?",
        "Options": [
            "a) pig {-param param_name = param_value | -param_file file_name} [-debug | -dryrun] script",
            "b) {%declare | %default} param_name param_value",
            "c) {%declare | %default} param_name param_value cmd",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 356,
        "Question": "You can specify parameter names and parameter values in one of the ways?",
        "Options": [
            "a) As part of a command line.",
            "b) In parameter file, as part of a command line",
            "c) With the declare statement, as part of Pig script",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 357,
        "Question": "_________ are scanned in the order they are specified on the command line.",
        "Options": [
            "a) Command line parameters",
            "b) Parameter files",
            "c) Declare and default preprocessors",
            "d) Both parameter files and command line parameters",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 358,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) LoadPredicatePushdown is same as LoadMetadata.setPartitionFilter",
            "b) getOutputFormat() is called by Pig to get the InputFormat used by the loader",
            "c) Pig works with data from many sources",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 359,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Pig can invoke code in language like Java Only",
            "b) Pig enables data workers to write complex data transformations without knowing Java",
            "c) Pig’s simple SQL-like scripting language is called Pig Latin, and appeals to developers already familiar with scripting languages and SQL",
            "d) Pig is complete, so you can do all required data manipulations in Apache Hadoop with Pig",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 360,
        "Question": "Pig Latin is _______ and fits very naturally in the pipeline paradigm while SQL is instead declarative.",
        "Options": [
            "a) functional",
            "b) procedural",
            "c) declarative",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 361,
        "Question": "In comparison to SQL, Pig uses ______________",
        "Options": [
            "a) Lazy evaluation",
            "b) ETL",
            "c) Supports pipeline splits",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 362,
        "Question": "Which of the following is an entry in jobconf?",
        "Options": [
            "a) pig.job",
            "b) pig.input.dirs",
            "c) pig.feature",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 363,
        "Question": "Which of the following command sets the value of a particular configuration variable (key)?",
        "Options": [
            "a) set -v",
            "b) set <key>=<value>",
            "c) set",
            "d) reset",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 364,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hive Commands are non-SQL statement such as setting a property or adding a resource",
            "b) Set -v prints a list of configuration variables that are overridden by the user or Hive",
            "c) Set sets a list of variables that are overridden by the user or Hive",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 365,
        "Question": "Which of the following operator executes a shell command from the Hive shell?",
        "Options": [
            "a) |",
            "b) !",
            "c) ^",
            "d) +",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 366,
        "Question": "Which of the following will remove the resource(s) from the distributed cache?",
        "Options": [
            "a) delete FILE[S] <filepath>*",
            "b) delete JAR[S] <filepath>*",
            "c) delete ARCHIVE[S] <filepath>*",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 367,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) source FILE <filepath> executes a script file inside the CLI",
            "b) bfs <bfs command> executes a dfs command from the Hive shell",
            "c) hive is Query language similar to SQL",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 368,
        "Question": "_________ is a shell utility which can be used to run Hive queries in either interactive or batch mode.",
        "Options": [
            "a) $HIVE/bin/hive",
            "b) $HIVE_HOME/hive",
            "c) $HIVE_HOME/bin/hive",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 369,
        "Question": "Which of the following is a command line option?",
        "Options": [
            "a) -d,–define <key=value>",
            "b) -e,–define <key=value>",
            "c) -f,–define <key=value>",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 370,
        "Question": "Which is the additional command line option is available in Hive 0.10.0?",
        "Options": [
            "a) –database <dbname>",
            "b) –db <dbname>",
            "c) –dbase <<dbname>",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 371,
        "Question": "The CLI when invoked without the -i option will attempt to load $HIVE_HOME/bin/.hiverc and $HOME/.hiverc as _______ files.",
        "Options": [
            "a) processing",
            "b) termination",
            "c) initialization",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 372,
        "Question": "When $HIVE_HOME/bin/hive is run without either the -e or -f option, it enters _______ mode.",
        "Options": [
            "a) Batch",
            "b) Interactive shell",
            "c) Multiple",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 373,
        "Question": "Hive uses _________ for logging.",
        "Options": [
            "a) logj4",
            "b) log4l",
            "c) log4i",
            "d) log4j",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 374,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) list FILE[S] <filepath>*  executes a Hive query and prints results to standard output",
            "b) <query string> executes a Hive query and prints results to standard output",
            "c) <query> executes a Hive query and prints results to standard output",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 375,
        "Question": "HiveServer2 introduced in Hive 0.11 has a new CLI called __________",
        "Options": [
            "a) BeeLine",
            "b) SqlLine",
            "c) HiveLine",
            "d) CLilLine",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 376,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) There are four namespaces for variables in Hive",
            "b) Custom variables can be created in a separate namespace with the define",
            "c) Custom variables can also be created in a separate namespace with hivevar",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 377,
        "Question": "HCatalog is installed with Hive, starting with Hive release is ___________",
        "Options": [
            "a) 0.10.0",
            "b) 0.9.0",
            "c) 0.11.0",
            "d) 0.12.0",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 378,
        "Question": "hiveconf variables are set as normal by using the following statement?",
        "Options": [
            "a) set -v x=myvalue",
            "b) set x=myvalue",
            "c) reset x=myvalue",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 379,
        "Question": "Variable Substitution is disabled by using ___________",
        "Options": [
            "a) set hive.variable.substitute=false;",
            "b) set hive.variable.substitutevalues=false;",
            "c) set hive.variable.substitute=true;",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 380,
        "Question": "_______ supports a new command shell Beeline that works with HiveServer2.",
        "Options": [
            "a) HiveServer2",
            "b) HiveServer3",
            "c) HiveServer4",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 381,
        "Question": "In ______ mode HiveServer2 only accepts valid Thrift calls.",
        "Options": [
            "a) Remote",
            "b) HTTP",
            "c) Embedded",
            "d) Interactive",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 382,
        "Question": "Hive specific commands can be run from Beeline, when the Hive _______ driver is used.",
        "Options": [
            "a) ODBC",
            "b) JDBC",
            "c) ODBC-JDBC",
            "d) All of the Mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 383,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) –helpusage display a usage message",
            "b) The JDBC connection URL format has the prefix jdbc:hive:",
            "c) Starting with Hive 0.14, there are improved SV output formats",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 384,
        "Question": "_________ reduce the amount of informational messages displayed (true) or not (false).",
        "Options": [
            "a) –silent=[true/false] ",
            "b) –autosave=[true/false] ",
            "c) –force=[true/false] ",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 385,
        "Question": "Which of the following is used to set transaction isolation level?",
        "Options": [
            "a) –incremental=[true/false] ",
            "b) –isolation=LEVEL",
            "c) –force=[true/false] ",
            "d) –truncateTable=[true/false] ",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 386,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HiveServer2 has a new JDBC driver",
            "b) CSV and TSV output formats are maintained for forward compatibility",
            "c) HiveServer2 supports both embedded and remote access to HiveServer2",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 387,
        "Question": "The ________ allows users to read or write Avro data as Hive tables.",
        "Options": [
            "a) AvroSerde",
            "b) HiveSerde",
            "c) SqlSerde",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 388,
        "Question": "Starting in Hive _______ the Avro schema can be inferred from the Hive table schema.",
        "Options": [
            "a) 0.14",
            "b) 0.12",
            "c) 0.13",
            "d) 0.11",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 389,
        "Question": "The AvroSerde has been built and tested against Hive 0.9.1 and later, and uses Avro _______ as of Hive 0.13 and 0.14.",
        "Options": [
            "a) 1.7.4",
            "b) 1.7.2",
            "c) 1.7.3",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 390,
        "Question": "Which of the following data type is supported by Hive?",
        "Options": [
            "a) map",
            "b) record",
            "c) string",
            "d) enum",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 391,
        "Question": "Which of the following data type is converted to Array prior to Hive 0.12.0?",
        "Options": [
            "a) map",
            "b) long",
            "c) float",
            "d) bytes",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 392,
        "Question": "Avro-backed tables can simply be created by using _________ in a DDL statement.",
        "Options": [
            "a) “STORED AS AVRO”",
            "b) “STORED AS HIVE”",
            "c) “STORED AS AVROHIVE”",
            "d) “STORED AS SERDE”",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 393,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Avro Fixed type should be defined in Hive as lists of tiny ints",
            "b) Avro Bytes type should be defined in Hive as lists of tiny ints",
            "c) Avro Enum type should be defined in Hive as strings",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 394,
        "Question": "Types that may be null must be defined as a ______ of that type and Null within Avro.",
        "Options": [
            "a) Union",
            "b) Intersection",
            "c) Set",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 395,
        "Question": "The files that are written by the _______ job are valid Avro files.",
        "Options": [
            "a) Avro",
            "b) Map Reduce",
            "c) Hive",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 396,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To create an Avro-backed table, specify the serde as org.apache.hadoop.hive.serde2.avro.AvroSerDe",
            "b) Avro-backed tables can be created in Hive using AvroSerDe",
            "c) The AvroSerde cannot serialize any Hive table to Avro files",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 397,
        "Question": "Use ________ and embed the schema in the create statement.",
        "Options": [
            "a) schema.literal",
            "b) schema.lit",
            "c) row.literal",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 398,
        "Question": "_______ is interpolated into the quotes to correctly handle spaces within the schema.",
        "Options": [
            "a) $SCHEMA",
            "b) $ROW",
            "c) $SCHEMASPACES",
            "d) $NAMESPACES",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 399,
        "Question": "To force Hive to be more verbose, it can be started with ___________",
        "Options": [
            "a) *hive –hiveconf hive.root.logger=INFO,console*",
            "b) *hive –hiveconf hive.subroot.logger=INFO,console*",
            "c) *hive –hiveconf hive.root.logger=INFOVALUE,console*",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 400,
        "Question": "________ was designed to overcome limitations of the other Hive file formats.",
        "Options": [
            "a) ORC",
            "b) OPC",
            "c) ODC",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 401,
        "Question": "An ORC file contains groups of row data called __________",
        "Options": [
            "a) postscript",
            "b) stripes",
            "c) script",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 402,
        "Question": "Serialization of string columns uses a ________ to form unique column values.",
        "Options": [
            "a) Footer",
            "b) STRIPES",
            "c) Dictionary",
            "d) Index",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 403,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The Avro file dump utility analyzes ORC files",
            "b) Streams are compressed using a codec, which is specified as a table property for all streams in that table",
            "c) The ODC file dump utility analyzes ORC files",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 404,
        "Question": "_______ is a lossless data compression library that favors speed over compression ratio.",
        "Options": [
            "a) LOZ",
            "b) LZO",
            "c) OLZ",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 405,
        "Question": "Which of the following will prefix the query string with parameters?",
        "Options": [
            "a) SET hive.exec.compress.output=false",
            "b) SET hive.compress.output=false",
            "c) SET hive.exec.compress.output=true",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 406,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) TIMESTAMP is Only available starting with Hive 0.10.0",
            "b) DECIMAL introduced in Hive 0.11.0 with a precision of 38 digits",
            "c) Hive 0.13.0 introduced user definable precision and scale",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 407,
        "Question": "Integral literals are assumed to be _________ by default.",
        "Options": [
            "a) SMALL INT",
            "b) INT",
            "c) BIG INT",
            "d) TINY INT",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 408,
        "Question": "Hive uses _____ style escaping within the strings.",
        "Options": [
            "a) C",
            "b) Java",
            "c) Python",
            "d) Scala",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 409,
        "Question": "Which of the following statement will create a column with varchar datatype?",
        "Options": [
            "a) CREATE TABLE foo (bar CHAR(10))",
            "b) CREATE TABLE foo (bar VARCHAR(10))",
            "c) CREATE TABLE foo (bar CHARVARYING(10))",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 410,
        "Question": "_________ will overwrite any existing data in the table or partition.",
        "Options": [
            "a) INSERT WRITE",
            "b) INSERT OVERWRITE",
            "c) INSERT INTO",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 411,
        "Question": "Hive does not support literals for ______ types.",
        "Options": [
            "a) Scalar",
            "b) Complex",
            "c) INT",
            "d) CHAR",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 412,
        "Question": "HBase is a distributed ________ database built on top of the Hadoop file system.",
        "Options": [
            "a) Column-oriented",
            "b) Row-oriented",
            "c) Tuple-oriented",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 413,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) HDFS provides low latency access to single rows from billions of records (Random access)",
            "b) HBase sits on top of the Hadoop File System and provides read and write access",
            "c) HBase is a distributed file system suitable for storing large files",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 414,
        "Question": "HBase is ________ defines only column families.",
        "Options": [
            "a) Row Oriented",
            "b) Schema-less",
            "c) Fixed Schema",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 415,
        "Question": "Apache HBase is a non-relational database modeled after Google’s _________",
        "Options": [
            "a) BigTop",
            "b) Bigtable",
            "c) Scanner",
            "d) FoundationDB",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 416,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HBase provides only sequential access to data",
            "b) HBase provides high latency batch processing",
            "c) HBase internally provides serialized access",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 417,
        "Question": "The _________ Server assigns regions to the region servers and takes the help of Apache ZooKeeper for this task.",
        "Options": [
            "a) Region",
            "b) Master",
            "c) Zookeeper",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 418,
        "Question": "Which of the following command provides information about the user?",
        "Options": [
            "a) status",
            "b) version",
            "c) whoami",
            "d) user",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 419,
        "Question": "Which of the following command does not operate on tables?",
        "Options": [
            "a) enabled",
            "b) disabled",
            "c) drop",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 420,
        "Question": "_________ command fetches the contents of a row or a cell.",
        "Options": [
            "a) select",
            "b) get",
            "c) put",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 421,
        "Question": "HBaseAdmin and ____________ are the two important classes in this package that provide DDL functionalities.",
        "Options": [
            "a) HTableDescriptor",
            "b) HDescriptor",
            "c) HTable",
            "d) HTabDescriptor",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 422,
        "Question": "The minimum number of row versions to keep is configured per column family via _____________",
        "Options": [
            "a) HBaseDecriptor",
            "b) HTabDescriptor",
            "c) HColumnDescriptor",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 423,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The default for max versions is 1",
            "b) It is recommended setting the number of max versions to an exceedingly high level",
            "c) HBase does overwrite row values",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 424,
        "Question": "HBase supports a ____________ interface via Put and Result.",
        "Options": [
            "a) “bytes-in/bytes-out”",
            "b) “bytes-in”",
            "c) “bytes-out”",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 425,
        "Question": "One supported data type that deserves special mention are ____________",
        "Options": [
            "a) money",
            "b) counters",
            "c) smallint",
            "d) tinyint",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 426,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Where time-ranges are very wide (e.g., year-long report) and where the data is voluminous, summary tables are a common approach",
            "b) Coprocessors act like RDBMS triggers",
            "c) HBase does not currently support ‘constraints’ in traditional (SQL) database parlance",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 427,
        "Question": "The _________ suffers from the monotonically increasing rowkey problem.",
        "Options": [
            "a) rowkey",
            "b) columnkey",
            "c) counterkey",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 428,
        "Question": "__________ does re-write data and pack rows into columns for certain time-periods.",
        "Options": [
            "a) OpenTS",
            "b) OpenTSDB",
            "c) OpenTSD",
            "d) OpenDB",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 429,
        "Question": "Which command is used to disable all the tables matching the given regex?",
        "Options": [
            "a) remove all",
            "b) drop all",
            "c) disable_all",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 430,
        "Question": "__________ command disables drops and recreates a table.",
        "Options": [
            "a) drop",
            "b) truncate",
            "c) delete",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 431,
        "Question": "Correct and valid syntax for count command is ____________",
        "Options": [
            "a) count ‘<row number>’",
            "b) count ‘<table name>’",
            "c) count ‘<column name>’",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 432,
        "Question": "_______ can change the maximum number of cells of a column family.",
        "Options": [
            "a) set",
            "b) reset",
            "c) alter",
            "d) select",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 433,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) You can add a column family to a table using the method addColumn()",
            "b) Using alter, you can also create a column family",
            "c) Using disable-all, you can truncate a column family",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 434,
        "Question": "Which of the following is not a table scope operator?",
        "Options": [
            "a) MEMSTORE_FLUSH",
            "b) MEMSTORE_FLUSHSIZE",
            "c) MAX_FILESIZE",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 435,
        "Question": "You can delete a column family from a table using the method _________ of HBAseAdmin class.",
        "Options": [
            "a) delColumn()",
            "b) removeColumn()",
            "c) deleteColumn()",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 436,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) To read data from an HBase table, use the get() method of the HTable class",
            "b) You can retrieve data from the HBase table using the get() method of the HTable class",
            "c) While retrieving data, you can get a single row by id, or get a set of rows by a set of row ids, or scan an entire table or a subset of rows",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 437,
        "Question": "__________ class adds HBase configuration files to its object.",
        "Options": [
            "a) Configuration",
            "b) Collector",
            "c) Component",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 438,
        "Question": "The ________ class provides the getValue() method to read the values from its instance.",
        "Options": [
            "a) Get",
            "b) Result",
            "c) Put",
            "d) Value",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 439,
        "Question": "________ communicate with the client and handle data-related operations.",
        "Options": [
            "a) Master Server",
            "b) Region Server",
            "c) Htable",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 440,
        "Question": "_________ is the main configuration file of HBase.",
        "Options": [
            "a) hbase.xml",
            "b) hbase-site.xml",
            "c) hbase-site-conf.xml",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 441,
        "Question": "HBase uses the _______ File System to store its data.",
        "Options": [
            "a) Hive",
            "b) Imphala",
            "c) Hadoop",
            "d) Scala",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 442,
        "Question": "ZooKeeper itself is intended to be replicated over a sets of hosts called ____________",
        "Options": [
            "a) chunks",
            "b) ensemble",
            "c) subdomains",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 443,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) ZooKeeper can achieve high throughput and high latency numbers",
            "b) The fault tolerant ordering means that sophisticated synchronization primitives can be implemented at the client",
            "c) The ZooKeeper implementation puts a premium on high performance, highly available, strictly ordered access",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 444,
        "Question": "Which of the guarantee is provided by Zookeeper?",
        "Options": [
            "a) Interactivity",
            "b) Flexibility",
            "c) Scalability",
            "d) Reliability",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 445,
        "Question": "ZooKeeper is especially fast in ___________ workloads.",
        "Options": [
            "a) write",
            "b) read-dominant",
            "c) read-write",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 446,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Distributed applications use SQL to store important configuration information",
            "b) The service maintains a record of all transactions, which can be used for higher-level abstractions, like synchronization primitives",
            "c) ZooKeeper maintains a standard hierarchical name space, similar to files and directories",
            "d) ZooKeeper provides superior reliability through redundant services",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 447,
        "Question": "When a _______ is triggered the client receives a packet saying that the znode has changed.",
        "Options": [
            "a) event",
            "b) watch",
            "c) row",
            "d) value",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 448,
        "Question": "The underlying client-server protocol has changed in version _______ of ZooKeeper.",
        "Options": [
            "a) 2.0.0",
            "b) 3.0.0",
            "c) 4.0.0",
            "d) 6.0.0",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 449,
        "Question": "The java package structure has changed from com.yahoo.zookeeper* to ___________",
        "Options": [
            "a) apache.zookeeper",
            "b) org.apache.zookeeper",
            "c) org.apache.zookeeper.package",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 450,
        "Question": "A number of constants used in the client ZooKeeper API were renamed in order to reduce ________ collision.",
        "Options": [
            "a) value",
            "b) namespace",
            "c) counter",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 451,
        "Question": "ZooKeeper allows distributed processes to coordinate with each other through registers, known as ___________",
        "Options": [
            "a) znodes",
            "b) hnodes",
            "c) vnodes",
            "d) rnodes",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 452,
        "Question": "Zookeeper essentially mirrors the _______ functionality exposed in the Linux kernel.",
        "Options": [
            "a) iread",
            "b) inotify",
            "c) iwrite",
            "d) icount",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 453,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) If we are to deploy Zookeeper in a distributed environment, we have to think about both the availability and scalability of the service",
            "b) Chubby and Hbase are both much more than a distributed lock service",
            "c) Given a cluster of Zookeeper servers, many can act as leader",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 454,
        "Question": " A ___________ server is a machine that keeps a copy of the state of the entire system and persists this information in local log files.",
        "Options": [
            "a) Master",
            "b) Region",
            "c) Zookeeper",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 455,
        "Question": "ZooKeeper’s architecture supports high ____________ through redundant services.",
        "Options": [
            "a) flexibility",
            "b) scalability",
            "c) availability",
            "d) interactivity",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 456,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Cluster-wide status centralization service is essential for management and serialization tasks across a large distributed set of servers",
            "b) Within ZooKeeper, an application can create what is called a znode",
            "c) The znode can be updated by any node in the cluster, and any node in the cluster can register to be informed of changes to that znode",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 457,
        "Question": "_________ serves distributed Lucene indexes in a grid environment.",
        "Options": [
            "a) Katta",
            "b) Helprace",
            "c) Neo4j",
            "d) 101tec",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 458,
        "Question": "The Email & Apps team of ___________ uses ZooKeeper to coordinate sharding and responsibility changes in a distributed email client.",
        "Options": [
            "a) Katta",
            "b) Helprace",
            "c) Rackspace",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 459,
        "Question": "ZooKeeper is used for configuration, leader election in Cloud edition of ______________",
        "Options": [
            "a) Solr",
            "b) Solur",
            "c) Solar101",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 460,
        "Question": "Helprace is using ZooKeeper on a _______ cluster in conjunction with Hadoop and HBase.",
        "Options": [
            "a) 3-node",
            "b) 4-node",
            "c) 5-node",
            "d) 6-node",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 461,
        "Question": "The ZooKeeper Data Directory contains files which are _________ copy of the znodes stored by a particular serving ensemble.",
        "Options": [
            "a) transient",
            "b) read only",
            "c) persistent",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 462,
        "Question": "You need to have _________ installed before running ZooKeeper.",
        "Options": [
            "a) Java",
            "b) C",
            "c) C++",
            "d) SQLGUI",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 463,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) All znodes are ephemeral, which means they are describing a “temporary” state",
            "b) /hbase/replication/state contains the list of RegionServers in the main cluster",
            "c) Offline snapshots are coordinated by the Master using ZooKeeper to communicate with the RegionServers using a two-phase-commit-like transaction",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 464,
        "Question": "How many types of special znodes are present in Zookeeper?",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 465,
        "Question": "To register a “watch” on a znode data, you need to use the _______ commands to access the current content or metadata.",
        "Options": [
            "a) stat",
            "b) put",
            "c) receive",
            "d) gets",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 466,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) All the znodes are prefixed using the default /hbase location",
            "b) ZooKeeper provides an interactive shell that allows you to explore the ZooKeeper state",
            "c) The znodes that you’ll most often see are the ones that coordinate operations like Region Assignment",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 467,
        "Question": "_______ has a design policy of using ZooKeeper only for transient data.",
        "Options": [
            "a) Hive",
            "b) Imphala",
            "c) Hbase",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 468,
        "Question": "Zookeeper keep track of the cluster state such as the ______ table location.",
        "Options": [
            "a) DOMAIN",
            "b) NODE",
            "c) ROOT",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 469,
        "Question": "The ________ master will register its own address in this znode at startup, making this znode the source of truth for identifying which server is the Master.",
        "Options": [
            "a) active",
            "b) passive",
            "c) region",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 470,
        "Question": "___________ is used to decommission more than one RegionServer at a time by creating sub-znodes.",
        "Options": [
            "a) /hbase/master",
            "b) /hbase/draining",
            "c) /hbase/passive",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 471,
        "Question": "The ______ znode is used for synchronizing the changes made to the _acl_ table by the grant/revoke commands.",
        "Options": [
            "a) zcl",
            "b) acl",
            "c) scl",
            "d) bnl",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 472,
        "Question": "BigDecimal is comprised of a ________ with an integer ‘scale’ field.",
        "Options": [
            "a) BigInt",
            "b) BigInteger",
            "c) MediumInt",
            "d) SmallInt",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 473,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) BooleanSerializer is used to parse string representations of boolean values into boolean scalar types",
            "b) BlobRef is a wrapper that holds a BLOB either directly",
            "c) BooleanParse is used to parse string representations of boolean values into boolean scalar types",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 474,
        "Question": "ClobRef is a wrapper that holds a CLOB either directly or a reference to a file that holds the ______ data.",
        "Options": [
            "a) CLOB",
            "b) BLOB",
            "c) MLOB",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 475,
        "Question": "__________ encapsulates a set of delimiters used to encode a record.",
        "Options": [
            "a) LargeObjectLoader",
            "b) FieldMapProcessor",
            "c) DelimiterSet",
            "d) LobSerializer",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 476,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Abstract base class that holds a reference to a Blob or a Clob",
            "b) ACCESSORTYPE is the type used to access this data in a streaming fashion",
            "c) CONTAINERTYPE is the type used to hold this data (e.g., BytesWritable)",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 477,
        "Question": "_________ supports null values for all types.",
        "Options": [
            "a) SmallObjectLoader",
            "b) FieldMapProcessor",
            "c) DelimiterSet",
            "d) JdbcWritableBridge",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 478,
        "Question": "Which of the following is a singleton instance class?",
        "Options": [
            "a) LargeObjectLoader",
            "b) FieldMapProcessor",
            "c) DelimiterSet",
            "d) LobSerializer",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 479,
        "Question": "Which of the following class is used for general processing of error?",
        "Options": [
            "a) LargeObjectLoader",
            "b) ProcessingException",
            "c) DelimiterSet",
            "d) LobSerializer",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 480,
        "Question": "Records are terminated by a __________ character.",
        "Options": [
            "a) RECORD_DELIMITER",
            "b) FIELD_DELIMITER",
            "c) FIELD_LIMITER",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 481,
        "Question": "10.The fields parsed by ____________ are backed by an internal buffer.",
        "Options": [
            "a) LargeObjectLoader",
            "b) ProcessingException",
            "c) RecordParser",
            "d) None of the Mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 482,
        "Question": "Which of the following interface is implemented by Sqoop for recording?",
        "Options": [
            "a) SqoopWrite",
            "b) SqoopRecord",
            "c) SqoopRead",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 483,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Interface FieldMapping is used for mapping of field",
            "b) Interface FieldMappable is used for mapping of field",
            "c) Sqoop is nothing but NoSQL to Hadoop",
            "d) Sqoop internally uses ODBC interface so it should work with any JDBC compatible database",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 484,
        "Question": "Sqoop is an open source tool written at ________",
        "Options": [
            "a) Cloudera",
            "b) IBM",
            "c) Microsoft",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 485,
        "Question": "Sqoop uses _________ to fetch data from RDBMS and stores that on HDFS.",
        "Options": [
            "a) Hive",
            "b) Map reduce",
            "c) Imphala",
            "d) BigTOP",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 486,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Sqoop is used to import complete database",
            "b) Sqoop is used to import selected columns from a particular table",
            "c) Sqoop is used to import selected tables",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 487,
        "Question": "_________ allows users to specify the target location inside of Hadoop.",
        "Options": [
            "a) Imphala",
            "b) Oozie",
            "c) Sqoop",
            "d) Hive",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 488,
        "Question": "Microsoft uses a Sqoop-based connector to help transfer data from _________ databases to Hadoop.",
        "Options": [
            "a) PostreSQL",
            "b) SQL Server",
            "c) Oracle",
            "d) MySQL",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 489,
        "Question": "__________ provides a Couchbase Server-Hadoop connector by means of Sqoop.",
        "Options": [
            "a) MemCache",
            "b) Couchbase",
            "c) Hbase",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 490,
        "Question": "Sqoop direct mode does not support imports of ______ columns.",
        "Options": [
            "a) BLOB",
            "b) LONGVARBINARY",
            "c) CLOB",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 491,
        "Question": "Sqoop has been tested with Oracle ______ Express Edition.",
        "Options": [
            "a) 11.2.0",
            "b) 10.2.0",
            "c) 12.2.0",
            "d) 10.3.0",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 492,
        "Question": "_________  tool can list all the available database schemas.",
        "Options": [
            "a) sqoop-list-tables",
            "b) sqoop-list-databases",
            "c) sqoop-list-schema",
            "d) sqoop-list-columns",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 493,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The sqoop command-line program is a wrapper which runs the bin/hadoop script shipped with Hadoop",
            "b) If $HADOOP_HOME is set, Sqoop will use the default installation location for Cloudera’s Distribution for Hadoop",
            "c) The active Hadoop configuration is loaded from $HADOOP_HOME/conf/, unless the $HADOOP_CONF_DIR environment variable is unset",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 494,
        "Question": "Data can be imported in maximum ______ file formats.",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 495,
        "Question": "________ text is appropriate for most non-binary data types.",
        "Options": [
            "a) Character",
            "b) Binary",
            "c) Delimited",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 496,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Avro data files are a compact, efficient binary format that provides interoperability with applications written in other programming languages",
            "b) By default, data is compressed while importing",
            "c) Delimited text also readily supports further manipulation by other tools, such as Hive",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 497,
        "Question": "If you set the inline LOB limit to ________ all large objects will be placed in external storage.",
        "Options": [
            "a) 0",
            "b) 1",
            "c) 2",
            "d) 3",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 498,
        "Question": "________ does not support the notion of enclosing characters that may include field delimiters in the enclosed string.",
        "Options": [
            "a) Imphala",
            "b) Oozie",
            "c) Sqoop",
            "d) Hive",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 499,
        "Question": "Sqoop can also import the data into Hive by generating and executing a ____________ statement to define the data’s layout in Hive.",
        "Options": [
            "a) SET TABLE",
            "b) CREATE TABLE",
            "c) INSERT TABLE",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 500,
        "Question": "The __________ tool imports a set of tables from an RDBMS to HDFS.",
        "Options": [
            "a) export-all-tables",
            "b) import-all-tables",
            "c) import-tables",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 501,
        "Question": "Which of the following argument is not supported by import-all-tables tool?",
        "Options": [
            "a) –class-name",
            "b) –package-name",
            "c) –database-name",
            "d) –table-name",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 502,
        "Question": "Apache Cassandra is a massively scalable open source _______ database.",
        "Options": [
            "a) SQL",
            "b) NoSQL",
            "c) NewSQL",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 503,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cassandra delivers continuous availability, linear scalability, and operational simplicity across many commodity servers",
            "b) Cassandra has a “masterless” architecture, meaning all nodes are the same",
            "c) Cassandra also provides customizable replication, storing redundant copies of data across nodes that participate in a Cassandra ring",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 504,
        "Question": "Cassandra uses a protocol called _______ to discover location and state information.",
        "Options": [
            "a) gossip",
            "b) intergos",
            "c) goss",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 505,
        "Question": "A __________ determines which data centers and racks nodes belong to it.",
        "Options": [
            "a) Client requests",
            "b) Snitch",
            "c) Partitioner",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 506,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Cassandra supplies linear scalability, meaning that capacity may be easily added simply by adding new nodes online",
            "b) Cassandra 2.0 included major enhancements to CQL, security, and performance",
            "c) CQL for Cassandra 2.0.6 adds several important features including batching of conditional updates, static columns, and increased control over slicing of clustering columns",
            "d) None of the Mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 507,
        "Question": "User accounts may be altered and dropped using the __________ Query Language.",
        "Options": [
            "a) Hive",
            "b) Cassandra",
            "c) Sqoop",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 508,
        "Question": "Authorization capabilities for Cassandra use the familiar _________ security paradigm to manage object permissions.",
        "Options": [
            "a) COMMIT",
            "b) GRANT",
            "c) ROLLBACK",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 509,
        "Question": "Client-to-node encryption protects data in flight from client machines to a database cluster using ___________",
        "Options": [
            "a) SSL",
            "b) SSH",
            "c) SSN",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 510,
        "Question": "Using ___________ file means you don’t have to override the SSL_CERTFILE environmental variables every time.",
        "Options": [
            "a) qlshrc",
            "b) cqshrc",
            "c) cqlshrc",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 511,
        "Question": "Internal authentication stores usernames and bcrypt-hashed passwords in the ____________ table.",
        "Options": [
            "a) system_auth.creds",
            "b) system_auth.credentials",
            "c) system.credentials",
            "d) sys_auth.credentials",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 512,
        "Question": "A _________ grants initial permissions, and subsequently a user may or may not be given the permission to grant/revoke permissions.",
        "Options": [
            "a) keyspace",
            "b) superuser",
            "c) sudouser",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 513,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cassandra accommodates expensive, consumer SSDs extremely well",
            "b) Cassandra re-writes or re-reads existing data, and never overwrites the rows in place",
            "c) Cassandra uses a storage structure similar to a Log-Structured Merge Tree",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 514,
        "Question": "__________ is one of many possible IAuthorizer implementations and the one that stores permissions in the system_auth.permissions table to support all authorization-related CQL statements.",
        "Options": [
            "a) CassandraAuth",
            "b) CassandraAuthorizer",
            "c) CassAuthorizer",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 515,
        "Question": "Cassandra creates a ___________ for each table, which allows you to symlink a table to a chosen physical drive or data volume.",
        "Options": [
            "a) directory",
            "b) subdirectory",
            "c) domain",
            "d) path",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 516,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Cassandra provides fine-grained control of table storage on disk, writing tables to disk using separate table directories within each keyspace directory",
            "b) The hinted handoff feature and Cassandra conformance and conformance to the ACID",
            "c) Client utilities and application programming interfaces (APIs) for developing applications for data storage and retrieval are available",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 517,
        "Question": "When ___________ contents exceed a configurable threshold, the memtable data, which includes indexes, is put in a queue to be flushed to disk.",
        "Options": [
            "a) subtable",
            "b) memtable",
            "c) intable",
            "d) memorytable",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 518,
        "Question": "Data in the commit log is purged after its corresponding data in the memtable is flushed to an _________",
        "Options": [
            "a) SSHables",
            "b) SSTable",
            "c) Memtables",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 519,
        "Question": "For each SSTable, Cassandra creates _________ index.",
        "Options": [
            "a) memory",
            "b) partition",
            "c) in memory",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 520,
        "Question": "Cassandra marks data to be deleted using _________",
        "Options": [
            "a) tombstone",
            "b) combstone",
            "c) tenstone",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 521,
        "Question": "Tombstones exist for a configured time period defined by the _______ value set on the table.",
        "Options": [
            "a) gc_grace_minutes",
            "b) gc_grace_time",
            "c) gc_grace_seconds",
            "d) gc_grace_hours",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 522,
        "Question": "_________ is a Cassandra feature that optimizes the cluster consistency process.",
        "Options": [
            "a) Hinted handon",
            "b) Hinted handoff",
            "c) Tombstone",
            "d) Hinted tomb",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 523,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cassandra does not immediately remove data marked for deletion from disk",
            "b) A deleted column can reappear if you do not run node repair routinely",
            "c) The deletion of marked data occurs during compaction",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 524,
        "Question": "Cassandra searches the __________ to determine the approximate location on disk of the index entry.",
        "Options": [
            "a) partition record",
            "b) partition summary",
            "c) partition search",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 525,
        "Question": "You configure sample frequency by changing the ________ property in the table definition.",
        "Options": [
            "a) index_time",
            "b) index_interval",
            "c) index_secs",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 526,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) A hint indicates that a write needs to be replayed to one or more unavailable nodes",
            "b) When the cluster cannot meet the consistency level specified by the client, Cassandra does store a hint",
            "c) By default, hints are saved for three hours after a replica fails because if the replica is down longer than that, it is likely permanently dead",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 527,
        "Question": "The compression offset map grows to ____ GB per terabyte compressed.",
        "Options": [
            "a) 1-3",
            "b) 10-16",
            "c) 20-22",
            "d) 0-1",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 528,
        "Question": "The type of __________ strategy Cassandra performs on your data is configurable and can significantly affect read performance.",
        "Options": [
            "a) compression",
            "b) collection",
            "c) compaction",
            "d) decompression",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 529,
        "Question": "There are _________ types of read requests that a coordinator can send to a replica.",
        "Options": [
            "a) two",
            "b) three",
            "c) four",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 530,
        "Question": "_________ can be configured per table for non-QUORUM consistency levels.",
        "Options": [
            "a) Read repair",
            "b) Read damage",
            "c) Write repair",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 531,
        "Question": "If the table has been configured with the __________ property, the coordinator node for the read request will retry the request with another replica node.",
        "Options": [
            "a) rapid_retry",
            "b) speculative_retry",
            "c) speculative_rapid",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 532,
        "Question": "BatchEE projects aims to provide an _________ implementation. (aka JSR352)",
        "Options": [
            "a) JBat",
            "b) JBatch",
            "c) JBash",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 533,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Blur is a search platform capable of searching massive amounts of data in a cloud computing environment",
            "b) Calcite is a not a very good customizable engine for parsing",
            "c) Broklyn is a highly customizable engine for parsing",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 534,
        "Question": "_________  allows database-like access, and in particular a SQL interface.",
        "Options": [
            "a) JBatch",
            "b) Calcite",
            "c) Blur",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 535,
        "Question": "___________ is a toolkit/application for converting between and editing common office file formats.",
        "Options": [
            "a) Droids",
            "b) DataFu",
            "c) Corinthia",
            "d) Ignite",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 536,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Droids aims to be an intelligent standalone robot framework that allows to create and extend existing droids",
            "b) HTrace is a tracing framework intended for use with distributed systems written in java",
            "c) DataFu provides a collection of Hadoop MapReduce jobs and functions in higher level languages",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 537,
        "Question": "Ignite is a unified ______ data fabric providing high-performance, distributed in-memory data management.",
        "Options": [
            "a) Column",
            "b) In-Memory",
            "c) Row oriented",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 538,
        "Question": "_________ is a distributed and scalable OLAP engine built on Hadoop to support extremely large datasets.",
        "Options": [
            "a) Kylin",
            "b) Lens",
            "c) log4cxx2",
            "d) MRQL",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 539,
        "Question": "NiFi is a dataflow system based on the concepts of ________ programming.",
        "Options": [
            "a) structured",
            "b) relational",
            "c) set",
            "d) flow-based",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 540,
        "Question": "__________ is a columnar storage format for Hadoop.",
        "Options": [
            "a) Ranger",
            "b) Parquet",
            "c) REEF",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 541,
        "Question": "Ripple is a browser based mobile phone emulator designed to aid in the development of _______ based mobile applications.",
        "Options": [
            "a) Javascript",
            "b) Java",
            "c) C++",
            "d) HTML5",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 542,
        "Question": "____________ is a query processing and optimization system for large-scale.",
        "Options": [
            "a) MRQL",
            "b) NiFi",
            "c) OpenAz",
            "d) ODF Toolkit",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 543,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) SAMOA provides a collection of distributed streaming algorithms",
            "b) REEF is a cross platform and cross runtime testing/debugging tool",
            "c) Sentry is a highly modular system for providing fine grained role",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 544,
        "Question": "________ is a columnar storage format for Hadoop.",
        "Options": [
            "a) MRQL",
            "b) NiFi",
            "c) OpenAz",
            "d) Parquet",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 545,
        "Question": "REEF is a scale-out computing fabric that eases the development of Big Data applications are ___________",
        "Options": [
            "a) MRQL",
            "b) NiFi",
            "c) REEF",
            "d) Ripple",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 546,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) OpenAz is a browser based mobile phone emulator",
            "b) Ripple is a cross platform and cross runtime testing/debugging tool",
            "c) Ripple currently supports such runtimes as Cordova, WebWorks and the Mobile Web",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 547,
        "Question": "Which of the following is a monitoring solution for hadoop?",
        "Options": [
            "a) Sirona",
            "b) Sentry",
            "c) Slider",
            "d) Streams",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 548,
        "Question": "Apache ________ is a lightweight server for ActivityStreams.",
        "Options": [
            "a) Sirona",
            "b) Taverna",
            "c) Slider",
            "d) Streams",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 549,
        "Question": "Which of the following provides extendible modern and functional API leveraging SE, ME and EE environments?",
        "Options": [
            "a) Sirona",
            "b) Taverna",
            "c) Tamaya",
            "d) Streams",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 550,
        "Question": "__________ is an abstraction over Apache Hadoop YARN that reduces the complexity of developing distributed applications.",
        "Options": [
            "a) Wave",
            "b) Twill",
            "c) Usergrid",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 551,
        "Question": "A _________ is a hosted, live, concurrent data structure for rich communication.",
        "Options": [
            "a) Wave",
            "b) Twill",
            "c) Usergrid",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 552,
        "Question": "Which of the following are a collaborative data analytics and visualization tool?",
        "Options": [
            "a) ACE",
            "b) Abdera",
            "c) Zeppelin",
            "d) Accumulo",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 553,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Buildr is a simple and intuitive build system for Java projects written in Ruby",
            "b) Celix is an OSGi like implementation in C with a distinct focus on interoperability between Java and C",
            "c) The Bean Validation project will create an implementation of Bean Validation as defined by the Java EE specifications",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 554,
        "Question": "_____________ is a software distribution framework based on OSGi.",
        "Options": [
            "a) ACE",
            "b) Abdera",
            "c) Zeppelin",
            "d) Accumulo",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 555,
        "Question": "___________ forge software for the development of software projects.",
        "Options": [
            "a) Oozie",
            "b) Allura",
            "c) Ambari",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 556,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Ambari is a monitoring, administration and lifecycle management project for Apache Hadoop clusters",
            "b) The Amber project will deliver a Java development framework mainly aimed to build OAuth-aware applications",
            "c) Bigtop is a project for the development of packaging and tests of the Hadoop ecosystem",
            "d) All of the Mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 557,
        "Question": "___________ is a software development collaboration tool.",
        "Options": [
            "a) Buildr",
            "b) Cassandra",
            "c) Bloodhound",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 558,
        "Question": "_____________ is an IaaS (“Infrastracture as a Service”) cloud orchestration platform.",
        "Options": [
            "a) CloudStack",
            "b) Cazerra",
            "c) Click",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 559,
        "Question": "Apache __________  is a platform for building native mobile applications using HTML, CSS and JavaScript (formerly Phonegap).",
        "Options": [
            "a) Cazerra",
            "b) Cordova",
            "c) CouchDB",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 560,
        "Question": "___________ is a Java library for writing, testing, and running pipelines of MapReduce jobs on Apache Hadoop.",
        "Options": [
            "a) cTakes",
            "b) Crunch",
            "c) CouchDB",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 561,
        "Question": "Which of the following project will create an SOA services framework?",
        "Options": [
            "a) DeltaCloud",
            "b) CXF",
            "c) DeltaSpike",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 562,
        "Question": "________ includes a ﬂexible and powerful toolkit for displaying monitoring and analyzing results.",
        "Options": [
            "a) Imphala",
            "b) Chukwa",
            "c) BigTop",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 563,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Log processing was one of the original purposes of MapReduce",
            "b) Chukwa is a Hadoop subproject devoted to bridging that gap between logs processing and Hadoop ecosystem",
            "c) HICC stands for Hadoop Infrastructure Care Center",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 564,
        "Question": "The items stored on _______ are organized in a hierarchy of widget category.",
        "Options": [
            "a) HICE",
            "b) HICC",
            "c) HIEC",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 565,
        "Question": "HICC, the Chukwa visualization interface, requires HBase version _____________",
        "Options": [
            "a) 0.90.5+.",
            "b) 0.10.4+.",
            "c) 0.90.4+.",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 566,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Using Hadoop for MapReduce processing of logs is easy",
            "b) Chukwa should work on any POSIX platform",
            "c) Chukwa is a system for large-scale reliable log collection and processing with Hadoop",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 567,
        "Question": "__________ are the Chukwa processes that actually produce data.",
        "Options": [
            "a) Collectors",
            "b) Agents",
            "c) HBase Table",
            "d) HCatalog",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 568,
        "Question": "Chukwa ___________ are responsible for accepting incoming data from Agents, and storing the data.",
        "Options": [
            "a) HBase Table",
            "b) Agents",
            "c) Collectors",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 569,
        "Question": "For enabling streaming data to _________ chukwa collector writer class can be configured in chukwa-collector-conf.xml.",
        "Options": [
            "a) HCatalog",
            "b) HBase",
            "c) Hive",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 570,
        "Question": "By default, collector’s listen on port _________",
        "Options": [
            "a) 8008",
            "b) 8070",
            "c) 8080",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 571,
        "Question": "_________ class allows other programs to get incoming chunks fed to them over a socket by the collector.",
        "Options": [
            "a) PipelineStageWriter",
            "b) PipelineWriter",
            "c) SocketTeeWriter",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 572,
        "Question": "__________ runs Demux parsers inside for convert unstructured data to semi-structured data, then load the key value pairs to HBase table.",
        "Options": [
            "a) HCatWriter",
            "b) HBWriter",
            "c) HBaseWriter",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 573,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) chukwa supports two different reliability strategies",
            "b) chukwaCollector.asyncAcks.scantime affects how often collectors will check the filesystem for commits",
            "c) chukwaCollector.asyncAcks.scanperiod defaults to thrice the rotation interval",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 574,
        "Question": "The __________ streams chunks of data to HDFS, and write data in temp filename with .chukwa suffix.",
        "Options": [
            "a) LocalWriter",
            "b) SeqFileWriter",
            "c) SocketTeeWriter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 575,
        "Question": "Conceptually, each _________ emits a semi-infinite stream of bytes, numbered starting from zero.",
        "Options": [
            "a) Collector",
            "b) Adaptor",
            "c) Compactor",
            "d) LocalWriter",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 576,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Filters use the same syntax as the Dump command",
            "b) “RAW” will send the internal data of the Chunk, without any metadata, prefixed by its length encoded as a 32-bit int",
            "c) Specifying “WRITABLE” will cause the chunks to be written using Hadoop Writable serialization framework",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 577,
        "Question": "The _____________ allows external processes to watch the stream of chunks passing through the collector.",
        "Options": [
            "a) LocalWriter",
            "b) SeqFileWriter",
            "c) SocketTeeWriter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 578,
        "Question": "Data analytics scripts are written in ____________",
        "Options": [
            "a) Hive",
            "b) CQL",
            "c) PigLatin",
            "d) Java",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 579,
        "Question": "If demux is successful within ____________ attempts, archives the completed files in Chukwa.",
        "Options": [
            "a) one",
            "b) two",
            "c) three",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 580,
        "Question": "Chukwa is ___________ data collection system for managing large distributed systems.",
        "Options": [
            "a) open source",
            "b) proprietary",
            "c) service based",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 581,
        "Question": "Collectors write chunks to logs/*.chukwa files until a __________ MB chunk is reached.",
        "Options": [
            "a) 64",
            "b) 108",
            "c) 256",
            "d) 1024",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 582,
        "Question": "___________ provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.",
        "Options": [
            "a) Oozie",
            "b) Ambari",
            "c) Hive",
            "d) Imphala",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 583,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Ambari provides a dashboard for monitoring the health and status of the Hadoop cluster",
            "b) Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts",
            "c) Ambari handles configuration of Hadoop services for the cluster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 584,
        "Question": "Ambari leverages ________ for metrics collection.",
        "Options": [
            "a) Nagios",
            "b) Nagaond",
            "c) Ganglia",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 585,
        "Question": "Ambari leverages ___________ for system alerting and will send emails when your attention is needed.",
        "Options": [
            "a) Nagios",
            "b) Nagaond",
            "c) Ganglia",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 586,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Ambari Views framework was greatly improved to better support instantiating and loading custom views",
            "b) The Ambari shell is written is Java, and uses the Groovy bases Ambari REST client",
            "c) Ambari-Shell is distributed as a single-file executable jar",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 587,
        "Question": "A ________ is a way of extending Ambari that allows 3rd parties to plug in new resource types along with the APIs.",
        "Options": [
            "a) trigger",
            "b) view",
            "c) schema",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 588,
        "Question": "Ambari ___________ deliver a template approach to cluster deployment.",
        "Options": [
            "a) View",
            "b) Stack Advisor",
            "c) Blueprints",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 589,
        "Question": "___________ facilitates installation of Hadoop across any number of hosts.",
        "Options": [
            "a) API-driven installations",
            "b) Wizard-driven interface",
            "c) Extensible framework",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 590,
        "Question": "Ambari provides a ________  API that enables integration with existing tools, such as Microsoft System Center.",
        "Options": [
            "a) RestLess",
            "b) Web Service",
            "c) RESTful",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 591,
        "Question": "If Ambari Agent has any output in /var/log/ambari-agent/ambari-agent.out, it is indicative of a __________ problem.",
        "Options": [
            "a) Less Severe",
            "b) Significant",
            "c) Extremely Severe",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 592,
        "Question": "A fully secure Hadoop cluster needs ___________",
        "Options": [
            "a) SSH",
            "b) SSL",
            "c) Kerberos",
            "d) REST",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 593,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Knox is a stateless reverse proxy framework",
            "b) Knox also intercepts REST/HTTP calls and provides authentication",
            "c) Knox scales linearly by adding more Knox nodes as the load increases",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 594,
        "Question": "A __________ can route requests to multiple Knox instances.",
        "Options": [
            "a) collector",
            "b) load balancer",
            "c) comparator",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 595,
        "Question": "Knox provides perimeter _________ for Hadoop clusters.",
        "Options": [
            "a) reliability",
            "b) security",
            "c) flexibility",
            "d) fault tolerant",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 596,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Knox eliminates the need for client software or client configuration and thus simplifies the access model",
            "b) Simplified access entend Hadoop’s REST/HTTP services by encapsulating Kerberos within the cluster",
            "c) Knox intercepts web vulnerability removal and other security services through a series of extensible interceptor pipelines",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 597,
        "Question": "Knox integrates with prevalent identity management and _______ systems.",
        "Options": [
            "a) SSL",
            "b) SSO",
            "c) SSH",
            "d) Kerberos",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 598,
        "Question": "The easiest way to have an HDP cluster is to download the _____________",
        "Options": [
            "a) Hadoop",
            "b) Sandbox",
            "c) Dashboard",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 599,
        "Question": "Apache Knox Eliminates _______ edge node risks.",
        "Options": [
            "a) SSL",
            "b) SSO",
            "c) SSH",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 600,
        "Question": "Apache Knox accesses Hadoop Cluster over _________",
        "Options": [
            "a) HTTP",
            "b) TCP",
            "c) ICMP",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 601,
        "Question": "Apache Knox provides __________ REST API Access Point.",
        "Options": [
            "a) Single",
            "b) Double",
            "c) Multiple",
            "d) Zero",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 602,
        "Question": "Apache Hadoop Development Tools is an effort undergoing incubation at _________",
        "Options": [
            "a) ADF",
            "b) ASF",
            "c) HCC",
            "d) AFS",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 603,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) HDT tool allows you to allow working with only 1.1 version of Hadoop",
            "b) HDT tool allows you to allow working with multiple versions of Hadoop",
            "c) HDT tool allows you to allow working with multiple versions of Hadoop from multiple IDE",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 604,
        "Question": "HDT project works with eclipse version ________ and above.",
        "Options": [
            "a) 3.4",
            "b) 3.5",
            "c) 3.6",
            "d) 3.7",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 605,
        "Question": "HDT has been tested on __________ and Juno, and can work on Kepler as well.",
        "Options": [
            "a) Rainbow",
            "b) Indigo",
            "c) Indiavo",
            "d) Hadovo",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 606,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) There is support for creating Hadoop project in HDT",
            "b) HDT aims at bringing plugins in eclipse to simplify development on Hadoop platform",
            "c) HDT is based on eclipse plugin architecture and can  possibly support other versions like 0.23, CDH4 etc in next releases",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 607,
        "Question": "Which of the following tool is intended to be more compatible with HDT?",
        "Options": [
            "a) Git",
            "b) Juno",
            "c) Indigo",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 608,
        "Question": "Which of the following has the core Eclipse PDE tools for HDT development?",
        "Options": [
            "a) RVP",
            "b) RAP",
            "c) RBP",
            "d) RVP",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 609,
        "Question": "HDT provides plugin for inspecting ________ nodes.",
        "Options": [
            "a) LocalWriter",
            "b) HICC",
            "c) HDFS",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 610,
        "Question": "HDT is used for listing running Jobs on __________ Cluster.",
        "Options": [
            "a) MR",
            "b) Hive",
            "c) Pig",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 611,
        "Question": "HDT provides wizards for creating Java Classes for ___________",
        "Options": [
            "a) Mapper",
            "b) Reducer",
            "c) Driver",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 612,
        "Question": "Spark was initially started by ____________ at UC Berkeley AMPLab in 2009.",
        "Options": [
            "a) Mahek Zaharia",
            "b) Matei Zaharia",
            "c) Doug Cutting",
            "d) Stonebraker",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 613,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) RSS abstraction provides distributed task dispatching, scheduling, and basic I/O functionalities",
            "b) For cluster manager, Spark supports standalone Hadoop YARN",
            "c) Hive SQL is a component on top of Spark Core",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 614,
        "Question": "____________ is a component on top of Spark Core.",
        "Options": [
            "a) Spark Streaming",
            "b) Spark SQL",
            "c) RDDs",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 615,
        "Question": "Spark SQL provides a domain-specific language to manipulate ___________ in Scala, Java, or Python.",
        "Options": [
            "a) Spark Streaming",
            "b) Spark SQL",
            "c) RDDs",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 616,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS)",
            "b) Spark also supports a pseudo-distributed mode, usually used only for development or testing purposes",
            "c) Spark has over 465 contributors in 2014",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 617,
        "Question": "______________ leverages Spark Core fast scheduling capability to perform streaming analytics.",
        "Options": [
            "a) MLlib",
            "b) Spark Streaming",
            "c) GraphX",
            "d) RDDs",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 618,
        "Question": "____________ is a distributed machine learning framework on top of Spark.",
        "Options": [
            "a) MLlib",
            "b) Spark Streaming",
            "c) GraphX",
            "d) RDDs",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 619,
        "Question": "________ is a distributed graph processing framework on top of Spark.",
        "Options": [
            "a) MLlib",
            "b) Spark Streaming",
            "c) GraphX",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 620,
        "Question": "GraphX provides an API for expressing graph computation that can model the __________ abstraction.",
        "Options": [
            "a) GaAdt",
            "b) Spark Core",
            "c) Pregel",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 621,
        "Question": "Spark architecture is ___________ times as fast as Hadoop disk-based Apache Mahout and even scales better than Vowpal Wabbit.",
        "Options": [
            "a) 10",
            "b) 20",
            "c) 50",
            "d) 100",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 622,
        "Question": "Users can easily run Spark on top of Amazon’s __________",
        "Options": [
            "a) Infosphere",
            "b) EC2",
            "c) EMR",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 623,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Spark enables Apache Hive users to run their unmodified queries much faster",
            "b) Spark interoperates only with Hadoop",
            "c) Spark is a popular data warehouse solution running on top of Hadoop",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 624,
        "Question": "Spark runs on top of ___________ a cluster manager system which provides efficient resource isolation across distributed applications.",
        "Options": [
            "a) Mesjs",
            "b) Mesos",
            "c) Mesus",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 625,
        "Question": "Which of the following can be used to launch Spark jobs inside MapReduce?",
        "Options": [
            "a) SIM",
            "b) SIMR",
            "c) SIR",
            "d) RIS",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 626,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Spark is intended to replace, the Hadoop stack",
            "b) Spark was designed to read and write data from and to HDFS, as well as other storage systems",
            "c) Hadoop users who have already deployed or are planning to deploy Hadoop Yarn can simply run Spark on YARN",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 627,
        "Question": "Which of the following language is not supported by Spark?",
        "Options": [
            "a) Java",
            "b) Pascal",
            "c) Scala",
            "d) Python",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 628,
        "Question": "Spark is packaged with higher level libraries, including support for _________ queries.",
        "Options": [
            "a) SQL",
            "b) C",
            "c) C++",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 629,
        "Question": "Spark includes a collection over ________ operators for transforming data and familiar data frame APIs for manipulating semi-structured data.",
        "Options": [
            "a) 50",
            "b) 60",
            "c) 70",
            "d) 80",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 630,
        "Question": "Spark is engineered from the bottom-up for performance, running ___________ faster than Hadoop by exploiting in memory computing and other optimizations.",
        "Options": [
            "a) 100x",
            "b) 150x",
            "c) 200x",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 631,
        "Question": "Spark powers a stack of high-level tools including Spark SQL, MLlib for _________",
        "Options": [
            "a) regression models",
            "b) statistics",
            "c) machine learning",
            "d) reproductive research",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 632,
        "Question": "Apache Flume 1.3.0 is the fourth release under the auspices of Apache of the so-called ________ codeline.",
        "Options": [
            "a) NG",
            "b) ND",
            "c) NF",
            "d) NR",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 633,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Flume is a distributed, reliable, and available service",
            "b) Version 1.5.2 is the eighth Flume release as an Apache top-level project",
            "c) Flume 1.5.2 is production-ready software for integration with hadoop",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 634,
        "Question": "___________ was created to allow you to flow data from a source into your Hadoop environment.",
        "Options": [
            "a) Imphala",
            "b) Oozie",
            "c) Flume",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 635,
        "Question": "A ____________ is an operation on the stream that can transform the stream.",
        "Options": [
            "a) Decorator",
            "b) Source",
            "c) Sinks",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 636,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Version 1.4.0 is the fourth Flume release as an Apache top-level project",
            "b) Apache Flume 1.5.2 is a security and maintenance release that disables SSLv3 on all components in Flume that support SSL/TLS",
            "c) Flume is backwards-compatible with previous versions of the Flume 1.x codeline",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 637,
        "Question": "A number of ____________ source adapters give you the granular control to grab a specific file.",
        "Options": [
            "a) multimedia file",
            "b) text file",
            "c) image file",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 638,
        "Question": "____________  is used when you want the sink to be the input source for another operation.",
        "Options": [
            "a) Collector Tier Event",
            "b) Agent Tier Event",
            "c) Basic",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 639,
        "Question": "___________ is where you would land a flow (or possibly multiple flows joined together) into an HDFS-formatted file system.",
        "Options": [
            "a) Collector Tier Event",
            "b) Agent Tier Event",
            "c) Basic",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 640,
        "Question": "____________ sink can be a text file, the console display, a simple HDFS path, or a null bucket where the data is simply deleted.",
        "Options": [
            "a) Collector Tier Event",
            "b) Agent Tier Event",
            "c) Basic",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 641,
        "Question": "Flume deploys as one or more agents, each contained within its own instance of _________",
        "Options": [
            "a) JVM",
            "b) Channels",
            "c) Chunks",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 642,
        "Question": "___________ provides Java-based indexing and search technology.",
        "Options": [
            "a) Solr",
            "b) Lucene Core",
            "c) Lucy",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 643,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Building PyLucene requires GNU Make, a recent version of Ant capable of building Java Lucene and a C++ compiler",
            "b) PyLucene is supported on Mac OS X, Linux, Solaris and Windows",
            "c) Use of setuptools is recommended for Lucene",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 644,
        "Question": "___________ is a high performance search server built using Lucene Core.",
        "Options": [
            "a) Solr",
            "b) Lucene Core",
            "c) Lucy",
            "d) PyLucene",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 645,
        "Question": "____________ is a subproject with the aim of collecting and distributing free materials.",
        "Options": [
            "a) OSR",
            "b) OPR",
            "c) ORP",
            "d) ORS",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 646,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) PyLucene is a Lucene port",
            "b) PyLucene embeds a Java VM with Lucene into a Python process",
            "c) The PyLucene Python extension, a Python module called lucene is machine-generated by JCC",
            "d) PyLucene is built with JCC",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 647,
        "Question": "_______ is a Python port of the Core project.",
        "Options": [
            "a) Solr",
            "b) Lucene Core",
            "c) Lucy",
            "d) PyLucene",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 648,
        "Question": "The Lucene _________ is pleased to announce the availability of Apache Lucene 5.0.0 and Apache Solr 5.0.0.",
        "Options": [
            "a) PMC",
            "b) RPC",
            "c) CPM",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 649,
        "Question": "___________ is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",
        "Options": [
            "a) Lucene",
            "b) Oozie",
            "c) Lucy",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 650,
        "Question": "Lucene provides scalable, high-Performance indexing over ______  per hour on modern hardware.",
        "Options": [
            "a) 1 TB",
            "b) 150GB",
            "c) 10 GB",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 651,
        "Question": "Lucene index size is roughly _______ the size of text indexed.",
        "Options": [
            "a) 10%",
            "b) 20%",
            "c) 50%",
            "d) 70%",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 652,
        "Question": "All file access uses Java’s __________ APIs which give Lucene stronger index safety.",
        "Options": [
            "a) NIO.2",
            "b) NIO.3",
            "c) NIO.4",
            "d) NIO.5",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 653,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Every Lucene segment now stores a unique id per-segment and per-commit to aid in accurate replication of index files",
            "b) The default norms format now uses sparse encoding when appropriate",
            "c) Tokenizers and Analyzers no longer require Reader on init",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 654,
        "Question": "During merging, __________ now always checks the incoming segments for corruption before merging.",
        "Options": [
            "a) LocalWriter",
            "b) IndexWriter",
            "c) ReadWriter",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 655,
        "Question": "Heap usage during IndexWriter merging is also much lower with the new _________",
        "Options": [
            "a) LucCodec",
            "b) Lucene50Codec",
            "c) Lucene20Cod",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 656,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) ConcurScheduler detects whether the index is on SSD or not",
            "b) Memory index supports payloads",
            "c) Auto-IO-throttling has been added to ConcurrentMergeScheduler, to rate limit IO writes for each merge depending on incoming merge rate",
            "d) The default codec has an option to control BEST_SPEED or BEST_COMPRESSION for stored fields",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 657,
        "Question": "PostingsFormat now uses a __________ API when writing postings, just like doc values.",
        "Options": [
            "a) push",
            "b) pull",
            "c) read",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 658,
        "Question": "New ____________ type enables Indexing and searching of date ranges, particularly multi-valued ones.",
        "Options": [
            "a) RangeField",
            "b) DateField",
            "c) DateRangeField",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 659,
        "Question": "SolrJ now has first class support for __________ API.",
        "Options": [
            "a) Compactions",
            "b) Collections",
            "c) Distribution",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 660,
        "Question": "____________ Collection API  allows for even distribution of custom replica properties.",
        "Options": [
            "a) BALANUNIQUE",
            "b) BALANCESHARDUNIQUE",
            "c) BALANCEUNIQUE",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 661,
        "Question": "____________ can be used to generate stats over the results of arbitrary numeric functions.",
        "Options": [
            "a) stats.field",
            "b) sta.field",
            "c) stats.value",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 662,
        "Question": "How many types of modes are present in Hama?",
        "Options": [
            "a) 2",
            "b) 3",
            "c) 4",
            "d) 5",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 663,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) In local mode, nothing must be launched via the start scripts",
            "b) Distributed Mode is just like the “Pseudo Distributed Mode”",
            "c) Apache Hama is one of the under-hyped projects in the Hadoop ecosystem",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 664,
        "Question": "__________ is the default mode if you download Hama.",
        "Options": [
            "a) Local Mode",
            "b) Pseudo Distributed Mode",
            "c) Distributed Mode",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 665,
        "Question": "_________ mode is used when you just have a single server and want to launch all the daemon processes.",
        "Options": [
            "a) Local Mode",
            "b) Pseudo Distributed Mode",
            "c) Distributed Mode",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 666,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Apache Hama is not a pure Bulk Synchronous Parallel Engine",
            "b) Hama uses the Hadoop Core for RPC calls",
            "c) Apache Hama is optimized for massive scientific computations such as matrix, graph and network algorithms",
            "d) Hama is a relatively newer project than Hadoop",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 667,
        "Question": "Distributed Mode are mapped in the __________ file.",
        "Options": [
            "a) groomservers",
            "b) grervers",
            "c) grsvers",
            "d) groom",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 668,
        "Question": "The web UI provides information about ________ job statistics of the Hama cluster.",
        "Options": [
            "a) MPP",
            "b) BSP",
            "c) USP",
            "d) ISP",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 669,
        "Question": "Apache Hama provides complete clone of _________",
        "Options": [
            "a) Pragmatic",
            "b) Pregel",
            "c) ServePreg",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 670,
        "Question": "A __________ in a social graph is a group of people who interact frequently with each other and less frequently with others.",
        "Options": [
            "a) semi-cluster",
            "b) partial cluster",
            "c) full cluster",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 671,
        "Question": "Which of the following apache project is gaining a lot of traction steadily with the efforts of its committers?",
        "Options": [
            "a) Hama",
            "b) Hadoop",
            "c) Hive",
            "d) Pig",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 672,
        "Question": "Hama is a general ________________ computing engine on top of Hadoop.",
        "Options": [
            "a) BSP",
            "b) ASP",
            "c) MPP",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 673,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Apache Hama is a distributed computing framework based on Bulk Synchronous Parallel computing techniques for massive scientific computations",
            "b) Hama is a Top Level Project under the Apache Software Foundation",
            "c) BSP stands for Bulk Synchronous Parallel",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 674,
        "Question": "Hama was inspired by Google’s _________ large-scale graph computing framework.",
        "Options": [
            "a) Pragmatic",
            "b) Pregel",
            "c) Preghad",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 675,
        "Question": "Hama requires JRE _______ or higher and ssh to be set up between nodes in the cluster.",
        "Options": [
            "a) 1.6",
            "b) 1.7",
            "c) 1.8",
            "d) 2.0",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 676,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The major difference between Hadoop and Hama is map/reduce tasks can’t communicate with each other",
            "b) Hama follows master/slave pattern",
            "c) A JobTracker maps to a BSPMaster, TaskTracker maps to a GroomServer and Map/Reduce task maps to a BSPTask",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 677,
        "Question": "Hama consist of mainly ________ components for large scale processing of graphs.",
        "Options": [
            "a) two",
            "b) three",
            "c) four",
            "d) five",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 678,
        "Question": "________ is responsible for maintaining groom server status.",
        "Options": [
            "a) GroomServers",
            "b) BSPMaster",
            "c) Zookeeper",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 679,
        "Question": "A __________ server and a data node should be run on one physical node.",
        "Options": [
            "a) groom",
            "b) web",
            "c) client",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 680,
        "Question": "A ________ is used to manage the efficient barrier synchronization of the BSPPeers.",
        "Options": [
            "a) GroomServers",
            "b) BSPMaster",
            "c) Zookeeper",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 681,
        "Question": "Groom servers starts up with a ________ instance and an RPC proxy to contact the bsp master.",
        "Options": [
            "a) RPC",
            "b) BSPPeer",
            "c) LPC",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 682,
        "Question": "HCatalog supports reading and writing files in any format for which a ________ can be written.",
        "Options": [
            "a) SerDE",
            "b) SaerDear",
            "c) DocSear",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 683,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) HCat provides connectors for MapReduce",
            "b) Apache HCatalog provides table data access for CDH components such as Pig and MapReduce",
            "c) HCat makes Hive metadata available to users of other Hadoop tools like Pig, MapReduce and Hive",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 684,
        "Question": "Hive version ___________ is the first release that includes HCatalog.",
        "Options": [
            "a) 0.10.0",
            "b) 0.11.0",
            "c) 0.12.0",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 685,
        "Question": "HCatalog is built on top of the Hive metastore and incorporates Hive’s is ____________",
        "Options": [
            "a) DDL",
            "b) DML",
            "c) TCL",
            "d) DCL",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 686,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools",
            "b) There is Hive-specific interface for HCatalog",
            "c) Data is defined using HCatalog’s command line interface (CLI)",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 687,
        "Question": "The HCatalog interface for Pig consists of ____________ and HCatStorer, which implement the Pig load and store interfaces respectively.",
        "Options": [
            "a) HCLoader",
            "b) HCatLoader",
            "c) HCatLoad",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 688,
        "Question": "_____________ accepts a table to read data from and optionally a selection predicate to indicate which partitions to scan.",
        "Options": [
            "a) HCatOutputFormat",
            "b) HCatInputFormat",
            "c) OutputFormat",
            "d) InputFormat",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 689,
        "Question": "The HCatalog __________ supports all Hive DDL that does not require MapReduce to execute.",
        "Options": [
            "a) Powershell",
            "b) CLI",
            "c) CMD",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 690,
        "Question": "You can write to a single partition by specifying the partition key(s) and value(s) in the ___________ method.",
        "Options": [
            "a) setOutput",
            "b) setOut",
            "c) put",
            "d) get",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 691,
        "Question": "HCatalog supports the same data types as _________",
        "Options": [
            "a) Pig",
            "b) Hama",
            "c) Hive",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 692,
        "Question": "__________ is a REST API for HCatalog.",
        "Options": [
            "a) WebHCat",
            "b) WbHCat",
            "c) InpHCat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 693,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) There is no guaranteed read consistency when a partition is dropped",
            "b) Unpartitioned tables effectively have one default partition that must be created at table creation time",
            "c) Once a partition is created, records cannot be added to it, removed from it, or updated in it",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 694,
        "Question": "With HCatalog _________ does not need to modify the table structure.",
        "Options": [
            "a) Partition",
            "b) Columns",
            "c) Robert",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 695,
        "Question": "Sally in data processing uses __________ to cleanse and prepare the data.",
        "Options": [
            "a) Pig",
            "b) Hive",
            "c) HCatalog",
            "d) Impala",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 696,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The original name of WebHCat was Templeton",
            "b) Robert in client management uses Hive to analyze his clients’ results",
            "c) With HCatalog, HCatalog cannot send a JMS message that data is available",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 697,
        "Question": "For ___________ partitioning jobs, simply specifying a custom directory is not good enough.",
        "Options": [
            "a) static",
            "b) semi cluster",
            "c) dynamic",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 698,
        "Question": "___________  property allows us to specify a custom dir location pattern for all the writes, and will interpolate each variable.",
        "Options": [
            "a) hcat.dynamic.partitioning.custom.pattern",
            "b) hcat.append.limit",
            "c) hcat.pig.storer.external.location",
            "d) hcatalog.hive.client.cache.expiry.time",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 699,
        "Question": "HCatalog maintains a cache of _________ to talk to the metastore.",
        "Options": [
            "a) HiveServer",
            "b) HiveClients",
            "c) HCatClients",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 700,
        "Question": "On the write side, it is expected that the user pass in valid _________ with data correctly.",
        "Options": [
            "a) HRecords",
            "b) HCatRecos",
            "c) HCatRecords",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 701,
        "Question": "A float parameter, defaults to 0.0001f, which means we can deal with 1 error every __________ rows.",
        "Options": [
            "a) 1000",
            "b) 10000",
            "c) 1 million rows",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 702,
        "Question": "_________________ property allow users to override the expiry time specified.",
        "Options": [
            "a) hcat.desired.partition.num.splits",
            "b) hcatalog.hive.client.cache.expiry.time",
            "c) hcatalog.hive.client.cache.disabled",
            "d) hcat.append.limit",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 703,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The HCatLoader and HCatStorer interfaces are used with Pig scripts to read and write data in HCatalog-managed tables",
            "b) HCatalog is not thread safe",
            "c) HCatLoader is used with Pig scripts to read data from HCatalog-managed tables.",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 704,
        "Question": "____________ is used with Pig scripts to write data to HCatalog-managed tables.",
        "Options": [
            "a) HamaStorer",
            "b) HCatStam",
            "c) HCatStorer",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 705,
        "Question": "Hive does not have a data type corresponding to the ____________ type in Pig.",
        "Options": [
            "a) decimal",
            "b) short",
            "c) biginteger",
            "d) datetime",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 706,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Hive metastore lets you create tables without specifying a database",
            "b) Restrictions apply to the types of columns HCatLoader can read from HCatalog-managed tables",
            "c) If the table is partitioned, you can indicate which partitions to scan by immediately following the load statement with a partition filter statement",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 707,
        "Question": "_______________ method is used to include a projection schema, to specify the output fields.",
        "Options": [
            "a) OutputSchema",
            "b) setOut",
            "c) setOutputSchema",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 708,
        "Question": "The first call on the HCatOutputFormat must be ____________",
        "Options": [
            "a) setOutputSchema",
            "b) setOutput",
            "c) setOut",
            "d) OutputSchema",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 709,
        "Question": "___________ is the type supported for storing values in HCatalog tables.",
        "Options": [
            "a) HCatRecord",
            "b) HCatColumns",
            "c) HCatValues",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 710,
        "Question": "The output descriptor for the table to be written is created by calling ____________",
        "Options": [
            "a) OutputJobInfo.describe",
            "b) OutputJobInfo.create",
            "c) OutputJobInfo.put",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 711,
        "Question": "Which of the following Hive commands is not supported by HCatalog?",
        "Options": [
            "a) ALTER INDEX … REBUILD",
            "b) CREATE VIEW",
            "c) SHOW FUNCTIONS",
            "d) DROP TABLE",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 712,
        "Question": "Mahout provides ____________ libraries for common  and primitive Java collections.",
        "Options": [
            "a) Java",
            "b) Javascript",
            "c) Perl",
            "d) Python",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 713,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Mahout is distributed under a commercially friendly Apache Software license",
            "b) Mahout is a library of scalable machine-learning algorithms, implemented on top of Apache Hadoop® and using the MapReduce paradigm",
            "c) Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 714,
        "Question": "_________ does not restrict contributions to Hadoop based implementations.",
        "Options": [
            "a) Mahout",
            "b) Oozie",
            "c) Impala",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 715,
        "Question": "Mahout provides an implementation of a ______________ identification algorithm which scores collocations using log-likelihood ratio.",
        "Options": [
            "a) collocation",
            "b) compaction",
            "c) collection",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 716,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) ‘Taste’ collaborative-filtering recommender component of Mahout was originally a separate project and can run standalone without Hadoop",
            "b) Integration of Mahout with initiatives such as the Pregel-like Giraph are actively under discussion",
            "c) Calculating the LLR is very straightforward",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 717,
        "Question": "The tokens are passed through a Lucene ____________ to produce NGrams of the desired length.",
        "Options": [
            "a) ShngleFil",
            "b) ShingleFilter",
            "c) SingleFilter",
            "d) Collfilter",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 718,
        "Question": "The _________ collocation identifier is integrated into the process that is used to create vectors from sequence files of text keys and values.",
        "Options": [
            "a) lbr",
            "b) lcr",
            "c) llr",
            "d) lar",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 719,
        "Question": "____________ generates NGrams and counts frequencies for ngrams, head and tail subgrams.",
        "Options": [
            "a) CollocationDriver",
            "b) CollocDriver",
            "c) CarDriver",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 720,
        "Question": "A key of type ___________ is generated which is used later to join ngrams with their heads and tails in the reducer phase.",
        "Options": [
            "a) GramKey",
            "b) Primary",
            "c) Secondary",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 721,
        "Question": "________ phase merges the counts for unique ngrams or ngram fragments across multiple documents.",
        "Options": [
            "a) CollocCombiner",
            "b) CollocReducer",
            "c) CollocMerger",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 722,
        "Question": "Drill is designed from the ground up to support high-performance analysis on the ____________ data.",
        "Options": [
            "a) semi-structured",
            "b) structured",
            "c) unstructured",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 723,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Drill provides plug-and-play integration with existing Apache Hive",
            "b) Developers can use the sandbox environment to get a feel for the power and capabilities of Apache Drill by performing various types of queries",
            "c) Drill is inspired by Google Dremel",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 724,
        "Question": "___________ includes Apache Drill as part of the Hadoop distribution.",
        "Options": [
            "a) Impala",
            "b) MapR",
            "c) Oozie",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 725,
        "Question": "MapR __________ Solution Earns Highest Score in Gigaom Research Data Warehouse Interoperability Report.",
        "Options": [
            "a) SQL-on-Hadoop",
            "b) Hive-on-Hadoop",
            "c) Pig-on-Hadoop",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 726,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Hadoop is a prerequisite for Drill",
            "b) Drill tackles rapidly evolving application driven schemas and nested data structures",
            "c) Drill provides a single interface for structured and semi-structured data allowing you to readily query JSON files and HBase tables as easily as a relational table",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 727,
        "Question": "Drill integrates with  BI tools using a standard __________ connector.",
        "Options": [
            "a) JDBC",
            "b) ODBC",
            "c) ODBC-JDBC",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 728,
        "Question": "Drill analyze semi-structured/nested data coming from _________ applications.",
        "Options": [
            "a) RDBMS",
            "b) NoSQL",
            "c) NewSQL",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 729,
        "Question": "Apache _________ provides direct queries on self-describing and semi-structured data in files.",
        "Options": [
            "a) Drill",
            "b) Mahout",
            "c) Oozie",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 730,
        "Question": "Drill provides a __________ like internal data model to represent and process data.",
        "Options": [
            "a) XML",
            "b) JSON",
            "c) TIFF",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 731,
        "Question": "Drill also provides intuitive extensions to SQL to work with _______ data types.",
        "Options": [
            "a) simple",
            "b) nested",
            "c) int",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 732,
        "Question": "The Apache Crunch Java library provides a framework for writing, testing, and running ___________ pipelines.",
        "Options": [
            "a) MapReduce",
            "b) Pig",
            "c) Hive",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 733,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Scrunch’s Java API is centered around three interfaces that represent distributed datasets",
            "b) All of the other data transformation operations supported by the Crunch APIs are implemented in terms of three primitives",
            "c) A number of common Aggregator<V> implementations are provided in the Aggregators class",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 734,
        "Question": "For Scala users, there is the __________ API, which is built on top of the Java APIs.",
        "Options": [
            "a) Prunch",
            "b) Scrunch",
            "c) Hivench",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 735,
        "Question": "The Crunch APIs are modeled after _________  which is the library that Google uses for building data pipelines on top of their own implementation of MapReduce.",
        "Options": [
            "a) FlagJava",
            "b) FlumeJava",
            "c) FlakeJava",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 736,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Crunch pipeline written by the development team sessionizes a set of user logs generates are then processed by a diverse collection of Pig scripts and Hive queries",
            "b) Crunch pipelines provide a thin veneer on top of MapReduce",
            "c) Developers have access to low-level MapReduce APIs",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 737,
        "Question": "Crunch was designed for developers who understand __________ and want to use MapReduce effectively.",
        "Options": [
            "a) Java",
            "b) Python",
            "c) Scala",
            "d) Javascript",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 738,
        "Question": "Hive, Pig, and Cascading all use a _________ data model.",
        "Options": [
            "a) value centric",
            "b) columnar",
            "c) tuple-centric",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 739,
        "Question": "A __________ represents a distributed, immutable collection of elements of type T.",
        "Options": [
            "a) PCollect<T>",
            "b) PCollection<T>",
            "c) PCol<T>",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 740,
        "Question": "___________ executes the pipeline as a series of MapReduce jobs.",
        "Options": [
            "a) SparkPipeline",
            "b) MRPipeline",
            "c) MemPipeline",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 741,
        "Question": "__________ represent the logical computations of your Crunch pipelines.",
        "Options": [
            "a) DoFns",
            "b) DoFn",
            "c) ThreeFns",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 742,
        "Question": "PCollection, PTable, and PGroupedTable all support a __________ operation.",
        "Options": [
            "a) intersection",
            "b) union",
            "c) OR",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 743,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) StreamPipeline executes the pipeline in-memory on the client",
            "b) MemPipeline executes the pipeline by converting it to a series of Spark pipelines",
            "c) MapReduce framework approach makes it easy for the framework to serialize data from the client to the cluster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 744,
        "Question": "Crunch uses Java serialization to serialize the contents of all of the ______ in a pipeline definition.",
        "Options": [
            "a) Transient",
            "b) DoFns",
            "c) Configuration",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 745,
        "Question": "Inline DoFn that splits a line up into words is an inner class ____________",
        "Options": [
            "a) Pipeline",
            "b) MyPipeline",
            "c) ReadPipeline",
            "d) WritePipe",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 746,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) DoFns also have a number of helper methods for working with Hadoop Counters, all named increment",
            "b) The Crunch APIs contain a number of useful subclasses of DoFn that handle common data processing scenarios and are easier to write and test",
            "c) FilterFn class defines a single abstract method",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 747,
        "Question": "DoFns provide direct access to the __________ object that is used within a given Map or Reduce task via the getContext method.",
        "Options": [
            "a) TaskInputContext",
            "b) TaskInputOutputContext",
            "c) TaskOutputContext",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 748,
        "Question": "The top-level ___________ package contains three of the most important specializations in Crunch.",
        "Options": [
            "a) org.apache.scrunch",
            "b) org.apache.crunch",
            "c) org.apache.kcrunch",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 749,
        "Question": "The Avros class also has a _____ method for creating PTypes for POJOs using Avro’s reflection-based serialization mechanism.",
        "Options": [
            "a) spot",
            "b) reflects",
            "c) gets",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 750,
        "Question": "The ______________ class defines a configuration parameter named LINES_PER_MAP that controls how the input file is split.",
        "Options": [
            "a) NLineInputFormat",
            "b) InputLineFormat",
            "c) LineInputFormat",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 751,
        "Question": "The ________ class allows developers to exercise precise control over how data is partitioned, sorted, and grouped by the underlying execution engine.",
        "Options": [
            "a) Grouping",
            "b) GroupingOptions",
            "c) RowGrouping",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 752,
        "Question": "Which of the following project is interface definition language for hadoop?",
        "Options": [
            "a) Oozie",
            "b) Mahout",
            "c) Thrift",
            "d) Impala",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 753,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Thrift is developed for scalable cross-language services development",
            "b) Thrift includes a complete stack for creating clients and servers",
            "c) The top part of the Thrift stack is generated code from the Thrift definition",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 754,
        "Question": "__________ is used as a remote procedure call (RPC) framework for facebook.",
        "Options": [
            "a) Oozie",
            "b) Mahout",
            "c) Thrift",
            "d) Impala",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 755,
        "Question": "Which of the following is a straightforward binary format?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 756,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) With Thrift, it is not possible to define a service and change the protocol and transport without recompiling the code",
            "b) Thrift includes server infrastructure to tie protocols and transports together, like blocking, non-blocking, and multi threaded servers",
            "c) Thrift supports a number of protocols for service definition",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 757,
        "Question": "Which of the following is a more compact binary format?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 758,
        "Question": "Which of the following format is similar to TCompactProtocol?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 759,
        "Question": "________ is a write-only protocol that cannot be parsed by Thrift.",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 760,
        "Question": "Which of the following Uses JSON for encoding of data?",
        "Options": [
            "a) TCompactProtocol",
            "b) TDenseProtocol",
            "c) TBinaryProtocol",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 761,
        "Question": "_____________ is a human-readable text format to aid in debugging.",
        "Options": [
            "a) TMemory",
            "b) TDebugProtocol",
            "c) TBinaryProtocol",
            "d) TSimpleJSONProtocol",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 762,
        "Question": "_______ transport is required when using a non-blocking server.",
        "Options": [
            "a) TZlibTransport",
            "b) TFramedTransport",
            "c) TMemoryTransport",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 763,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) To create a Mahout service, one has to write Thrift files that describe it, generate the code in the destination language",
            "b) Thrift is written in Java",
            "c) Thrift is a lean and clean library",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 764,
        "Question": "__________ uses memory for I/O in Thrift.",
        "Options": [
            "a) TZlibTransport",
            "b) TFramedTransport",
            "c) TMemoryTransport",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 765,
        "Question": "________ uses blocking socket I/O for transport.",
        "Options": [
            "a) TNonblockingServer",
            "b) TSimpleServer",
            "c) TSocket",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 766,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) There are no XML configuration files in Thrift",
            "b) Thrift gives cross-language serialization with lower overhead than alternatives such as SOAP due to use of binary format",
            "c) No framework to code is a feature of Thrift",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 767,
        "Question": "Which of the following is a multi-threaded server using non-blocking I/O?",
        "Options": [
            "a) TNonblockingServer",
            "b) TSimpleServer",
            "c) TSocket",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 768,
        "Question": "__________ is a single-threaded server using standard blocking I/O.",
        "Options": [
            "a) TNonblockingServer",
            "b) TSimpleServer",
            "c) TSocket",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 769,
        "Question": "Which of the following performs compression using zlib?",
        "Options": [
            "a) TZlibTransport",
            "b) TFramedTransport",
            "c) TMemoryTransport",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 770,
        "Question": "________ is a multi-threaded server using standard blocking I/O.",
        "Options": [
            "a) TNonblockingServer",
            "b) TThreadPoolServer",
            "c) TSimpleServer",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 771,
        "Question": "_____________ transport writes to a file.",
        "Options": [
            "a) TNonblockingServer",
            "b) TFileTransport",
            "c) TFramedTransport",
            "d) TMemoryTransport",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 772,
        "Question": "__________ is a server based Bundle Engine that provides a higher-level oozie abstraction that will batch a set of coordinator applications.",
        "Options": [
            "a) Oozie v2",
            "b) Oozie v3",
            "c) Oozie v4",
            "d) Oozie v5",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 773,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Oozie is a scalable, reliable and extensible system",
            "b) Oozie is a server-based Workflow Engine specialized in running workflow jobs",
            "c) Oozie Coordinator jobs are recurrent Oozie Workflow jobs triggered by time (frequency) and data availability",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 774,
        "Question": "___________ is a Java Web application used to schedule Apache Hadoop jobs.",
        "Options": [
            "a) Impala",
            "b) Oozie",
            "c) Mahout",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 775,
        "Question": "Oozie Workflow jobs are Directed ________ graphs  of actions.",
        "Options": [
            "a) Acyclical",
            "b) Cyclical",
            "c) Elliptical",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 776,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Oozie v2 is a server based Coordinator Engine specialized in running workflows based on time and data triggers",
            "b) Oozie v1 is a server based Workflow Engine specialized in running workflow jobs with actions that execute Hadoop Map/Reduce and Pig jobs",
            "c) A Workflow application is DAG that coordinates the following types of actions",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 777,
        "Question": "Oozie v2 is a server based ___________ Engine specialized in running workflows based on time and data triggers.",
        "Options": [
            "a) Compactor",
            "b) Collector",
            "c) Coordinator",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 778,
        "Question": "Which of the following is one of the possible state for a workflow jobs?",
        "Options": [
            "a) PREP",
            "b) START",
            "c) RESUME",
            "d) END",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 779,
        "Question": "Oozie can make _________ callback notifications on action start events and workflow end events.",
        "Options": [
            "a) TCP",
            "b) HTTP",
            "c) IP",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 780,
        "Question": "A workflow definition is a ______ with control flow nodes or action nodes.",
        "Options": [
            "a) CAG",
            "b) DAG",
            "c) BAG",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 781,
        "Question": "Which of the following workflow definition language is XML based?",
        "Options": [
            "a) hpDL",
            "b) hDL",
            "c) hiDL",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 782,
        "Question": "___________ nodes are the mechanism by which a workflow triggers the execution of a computation/processing task.",
        "Options": [
            "a) Server",
            "b) Client",
            "c) Mechanism",
            "d) Action",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 783,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Oozie is a Java Web-Application that runs in a Java servlet-container",
            "b) Oozie workflow is a collection of actions arranged in a control dependency DAG",
            "c) hPDL is a fairly compact language",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 784,
        "Question": "Workflow with id __________ should be in SUCCEEDED/KILLED/FAILED.",
        "Options": [
            "a) wfId",
            "b) iUD",
            "c) iFD",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 785,
        "Question": "Nodes in the config _____________  must be completed successfully.",
        "Options": [
            "a) oozie.wid.rerun.skip.nodes",
            "b) oozie.wf.rerun.skip.nodes",
            "c) oozie.wf.run.skip.nodes",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 786,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Oozie provides a unique callback URL to the task, the task should invoke the given URL to notify its completion",
            "b) All computation/processing tasks triggered by an mechanism node are remote to Oozie",
            "c) Oozie workflows can be parameterized",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 787,
        "Question": "_____________ will skip the nodes given in the config with the same exit transition as before.",
        "Options": [
            "a) ActionMega handler",
            "b) Action handler",
            "c) Data handler",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 788,
        "Question": "________ nodes that control the start and end of the workflow and workflow job execution path.",
        "Options": [
            "a) Action",
            "b) Control",
            "c) Data",
            "d) SubDomain",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 789,
        "Question": "Node names and transitions must be conform to the following pattern =[a-zA-Z][\\-_a-zA-Z0-0]*=, of up to __________ characters long.",
        "Options": [
            "a) 10",
            "b) 15",
            "c) 20",
            "d) 25",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 790,
        "Question": "A workflow definition must have one ________ node.",
        "Options": [
            "a) start",
            "b) resume",
            "c) finish",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 791,
        "Question": "If one or more actions started by the workflow job are executing when the ________ node is reached, the actions will be killed.",
        "Options": [
            "a) kill",
            "b) start",
            "c) end",
            "d) finsih",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 792,
        "Question": "A ___________ node enables a workflow to make a selection on the execution path to follow.",
        "Options": [
            "a) fork",
            "b) decision",
            "c) start",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 793,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Predicates are JSP Expression Language (EL) expressions",
            "b) Predicates are evaluated in order or appearance until one of them evaluates to true and the corresponding transition is taken",
            "c) The name attribute in the decision node is the name of the decision node",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 794,
        "Question": "Which of the following can be seen as a switch-case statement?",
        "Options": [
            "a) fork",
            "b) decision",
            "c) start",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 795,
        "Question": "All decision nodes must have a _____________ element to avoid bringing the workflow into an error state if none of the predicates evaluates to true.",
        "Options": [
            "a) name",
            "b) default",
            "c) server",
            "d) client",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 796,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The fork and join nodes must be used in pairs",
            "b) The fork node assumes concurrent execution paths are children of the same fork node",
            "c) A join node waits until every concurrent execution path of a previous fork node arrives to it",
            "d) A fork node splits one path of execution into multiple concurrent paths of execution",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 797,
        "Question": "The ___________ attribute in the join node is the name of the workflow join node.",
        "Options": [
            "a) name",
            "b) to",
            "c) down",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 798,
        "Question": "If a computation/processing task -triggered by a workflow fails to complete successfully, its transitions to _____________",
        "Options": [
            "a) error",
            "b) ok",
            "c) true",
            "d) false",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 799,
        "Question": "If the failure is of ___________ nature, Oozie will suspend the workflow job.",
        "Options": [
            "a) transient",
            "b) non-transient",
            "c) permanent",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 800,
        "Question": "A _______________ action can be configured to perform file system cleanup and directory creation before starting the mapreduce job.",
        "Options": [
            "a) map",
            "b) reduce",
            "c) map-reduce",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 801,
        "Question": "___________ properties can be overridden by specifying them in the job-xml file or configuration element.",
        "Options": [
            "a) Pipe",
            "b) Decision",
            "c) Flag",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 802,
        "Question": "A collection of various actions in a control dependency DAG is referred to as ________________",
        "Options": [
            "a) workflow",
            "b) dataflow",
            "c) clientflow",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 803,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Large datasets are incentives for users to come to Hadoop",
            "b) Data management is a common concern to be offered as a service",
            "c) Understanding the life-time of a feed will allow for implicit validation of the processing rules",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 804,
        "Question": "The ability of Hadoop to efficiently process large volumes of data in parallel is called __________ processing.",
        "Options": [
            "a) batch",
            "b) stream",
            "c) time",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 805,
        "Question": "__________ is used for simplified Data Management in Hadoop.",
        "Options": [
            "a) Falcon",
            "b) flume",
            "c) Impala",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 806,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Falcon promotes Javascript Programming",
            "b) Falcon does not do any heavy lifting but delegates to tools with in the Hadoop ecosystem",
            "c) Falcon handles retry logic and late data processing. Records audit, lineage and metrics",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 807,
        "Question": "Falcon provides ___________ workflow for copying data from source to target.",
        "Options": [
            "a) recurring",
            "b) investment",
            "c) data",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 808,
        "Question": "A recurring workflow is used for purging expired data on __________ cluster.",
        "Options": [
            "a) Primary",
            "b) Secondary",
            "c) BCP",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 809,
        "Question": "Falcon provides the key services data processing applications need so Sophisticated________ can easily be added to Hadoop applications.",
        "Options": [
            "a) DAM",
            "b) DLM",
            "c) DCM",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 810,
        "Question": "Falcon promotes decoupling of data set location from ___________ definition.",
        "Options": [
            "a) Oozie",
            "b) Impala",
            "c) Kafka",
            "d) Thrift",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 811,
        "Question": "Falcon provides seamless integration with _____________",
        "Options": [
            "a) HCatalog",
            "b) metastore",
            "c) HBase",
            "d) Kafka",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 812,
        "Question": "Which of the following is project for Infrastructure Engineers and Data Scientists?",
        "Options": [
            "a) Impala",
            "b) BigTop",
            "c) Oozie",
            "d) Flume",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 813,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Bigtop provides an integrated smoke testing framework, alongside a suite of over 10 test files",
            "b) Bigtop includes tools and a framework for testing at various levels",
            "c) Bigtop components supports only one Operating Systems",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 814,
        "Question": "Which of the following work is done by BigTop in Hadoop framework?",
        "Options": [
            "a) Packaging",
            "b) Smoke Testing",
            "c) Virtualization",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 815,
        "Question": "Which of the following operating system is not supported by BigTop?",
        "Options": [
            "a) Fedora",
            "b) Solaris",
            "c) Ubuntu",
            "d) SUSE",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 816,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Bigtop-0.5.0 : Builds the 0.5.0 release",
            "b) Bigtop-trunk-HBase builds the HCatalog packages only",
            "c) There are also jobs for building virtual machine images",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 817,
        "Question": "Apache Bigtop uses ___________ for continuous integration testing.",
        "Options": [
            "a) Jenkinstop",
            "b) Jerry",
            "c) Jenkins",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 818,
        "Question": "The Apache Jenkins server runs the ______________ job whenever code is committed to the trunk branch.",
        "Options": [
            "a) “Bigtop-trunk”",
            "b) “Bigtop”",
            "c) “Big-trunk”",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 819,
        "Question": "The Bigtop Jenkins server runs daily jobs for the _______ and trunk branches.",
        "Options": [
            "a) 0.1",
            "b) 0.2",
            "c) 0.3",
            "d) 0.4",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 820,
        "Question": "Which of the following builds an APT or YUM package repository?",
        "Options": [
            "a) Bigtop-trunk-packagetest",
            "b) Bigtop-trunk-repository",
            "c) Bigtop-VM-matrix",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 821,
        "Question": "___________ builds virtual machines of branches trunk and 0.3 for KVM, VMWare and VirtualBox.",
        "Options": [
            "a) Bigtop-trunk-packagetest",
            "b) Bigtop-trunk-repository",
            "c) Bigtop-VM-matrix",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 822,
        "Question": "__________ is a fully integrated, state-of-the-art analytic database architected specifically to leverage strengths of Hadoop.",
        "Options": [
            "a) Oozie",
            "b) Impala",
            "c) Lucene",
            "d) BigTop",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 823,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) With Impala, more users, whether using SQL queries or BI applications, can interact with more data",
            "b) Technical support for Impala is not available via a Cloudera Enterprise subscription",
            "c) Impala is proprietary tool for Hadoop",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 824,
        "Question": "Impala is an integrated part of a ____________ enterprise data hub.",
        "Options": [
            "a) MicroSoft",
            "b) IBM",
            "c) Cloudera",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 825,
        "Question": "For Apache __________ users, Impala utilizes the same metadata.",
        "Options": [
            "a) cTakes",
            "b) Hive",
            "c) Pig",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 826,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) For Apache Hive users, Impala utilizes the same metadata, ODBC driver, SQL syntax, and user interface as Hive",
            "b) Impala provides high latency and low concurrency",
            "c) Impala also scales linearly, even in multi tenant environments",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 827,
        "Question": "Impala is integrated with native Hadoop security and Kerberos for authentication via __________ module.",
        "Options": [
            "a) Sentinue",
            "b) Sentry",
            "c) Sentinar",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 828,
        "Question": "Which of the following companies shipped Impala?",
        "Options": [
            "a) Amazon",
            "b) Oracle",
            "c) MapR",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 829,
        "Question": "____________ analytics is a work in progress with Impala.",
        "Options": [
            "a) Reproductive",
            "b) Exploratory",
            "c) Predictive",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 830,
        "Question": "Which of the following features is not provided by Impala?",
        "Options": [
            "a) SQL functionality",
            "b) ACID",
            "c) Flexibility",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 831,
        "Question": "Which of the following hadoop file formats is supported by Impala?",
        "Options": [
            "a) SequenceFile",
            "b) Avro",
            "c) RCFile",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 832,
        "Question": "____________ is a distributed real-time computation system for processing large volumes of high-velocity data.",
        "Options": [
            "a) Kafka",
            "b) Storm",
            "c) Lucene",
            "d) BigTop",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 833,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) A Storm topology consumes streams of data and processes those streams in arbitrarily complex ways",
            "b) Apache Storm is a free and open source distributed real-time computation system",
            "c) Storm integrates with the queueing and database technologies you already use",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 834,
        "Question": "Storm integrates with __________ via Apache Slider.",
        "Options": [
            "a) Scheduler",
            "b) YARN",
            "c) Compaction",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 835,
        "Question": "For Apache __________ users, Storm utilizes the same ODBC interface.",
        "Options": [
            "a) cTakes",
            "b) Hive",
            "c) Pig",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 836,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Storm is difficult and can be used with only Java",
            "b) Storm is fast: a benchmark clocked it at over a million tuples processed per second per node",
            "c) Storm is scalable, fault-tolerant, guarantees your data will be processed",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 837,
        "Question": "Storm is benchmarked as processing one million _______ byte messages per second per node.",
        "Options": [
            "a) 10",
            "b) 50",
            "c) 100",
            "d) 200",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 838,
        "Question": "Apache Storm added the open source, stream data processing to _________ Data Platform.",
        "Options": [
            "a) Cloudera",
            "b) Hortonworks",
            "c) Local Cloudera",
            "d) MapR",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 839,
        "Question": "How many types of nodes are present in Storm cluster?",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) 4",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 840,
        "Question": "__________  node distributes code across the cluster.",
        "Options": [
            "a) Zookeeper",
            "b) Nimbus",
            "c) Supervisor",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 841,
        "Question": "____________ communicates with Nimbus through Zookeeper, starts and stops workers according to signals from Nimbus.",
        "Options": [
            "a) Zookeeper",
            "b) Nimbus",
            "c) Supervisor",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 842,
        "Question": "Kafka is comparable to traditional messaging systems such as _____________",
        "Options": [
            "a) Impala",
            "b) ActiveMQ",
            "c) BigTop",
            "d) Zookeeper",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 843,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds",
            "b) Activity tracking is often very high volume as many activity messages are generated for each user page view",
            "c) Kafka is often used for operational monitoring data",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 844,
        "Question": "Many people use Kafka as a replacement for a ___________ solution.",
        "Options": [
            "a) log aggregation",
            "b) compaction",
            "c) collection",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 845,
        "Question": "_______________ is a style of application design where state changes are logged as a time-ordered sequence of records.",
        "Options": [
            "a) Event sourcing",
            "b) Commit Log",
            "c) Stream Processing",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 846,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Kafka can serve as a kind of external commit-log for a distributed system",
            "b) The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data",
            "c) Kafka comes with a command-line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 847,
        "Question": "Kafka uses __________ so you need to first start a ZooKeeper server if you don’t already have one.",
        "Options": [
            "a) Impala",
            "b) ActiveMQ",
            "c) BigTop",
            "d) Zookeeper",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 848,
        "Question": "__________ is the node responsible for all reads and writes for the given partition.",
        "Options": [
            "a) replicas",
            "b) leader",
            "c) follower",
            "d) isr",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 849,
        "Question": "__________ is the subset of the replicas list that is currently alive and caught up to the leader.",
        "Options": [
            "a) replicas",
            "b) leader",
            "c) follower",
            "d) isr",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 850,
        "Question": "Kafka uses key-value pairs in the ____________ file format for configuration.",
        "Options": [
            "a) RFC",
            "b) Avro",
            "c) Property",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 851,
        "Question": "__________ is the amount of time to keep a log segment before it is deleted.",
        "Options": [
            "a) log.cleaner.enable",
            "b) log.retention",
            "c) log.index.enable",
            "d) log.flush.interval.message",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 852,
        "Question": "__________ provides the functionality of a messaging system.",
        "Options": [
            "a) Oozie",
            "b) Kafka",
            "c) Lucene",
            "d) BigTop",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 853,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) With Kafka, more users, whether using SQL queries or BI applications, can interact with more data",
            "b) A topic is a category or feed name to which messages are published",
            "c) For each topic, the Kafka cluster maintains a partitioned log",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 854,
        "Question": "Kafka maintains feeds of messages in categories called __________",
        "Options": [
            "a) topics",
            "b) chunks",
            "c) domains",
            "d) messages",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 855,
        "Question": "Kafka is run as a cluster comprised of one or more servers each of which is called __________",
        "Options": [
            "a) cTakes",
            "b) broker",
            "c) test",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 856,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The Kafka cluster does not retain all published messages",
            "b) A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients",
            "c) Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization",
            "d) Messages are persisted on disk and replicated within the cluster to prevent data loss",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 857,
        "Question": "Communication between the clients and the servers is done with a simple, high-performance, language-agnostic _________ protocol.",
        "Options": [
            "a) IP",
            "b) TCP",
            "c) SMTP",
            "d) ICMP",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 858,
        "Question": "The only metadata retained on a per-consumer basis is the position of the consumer in the log, called __________",
        "Options": [
            "a) offset",
            "b) partition",
            "c) chunks",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 859,
        "Question": "Each kafka partition has one server which acts as the _________",
        "Options": [
            "a) leaders",
            "b) followers",
            "c) staters",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 860,
        "Question": "_________ has stronger ordering guarantees than a traditional messaging system.",
        "Options": [
            "a) kafka",
            "b) Slider",
            "c) Suz",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 861,
        "Question": "Kafka only provides a _________ order over messages within a partition.",
        "Options": [
            "a) partial",
            "b) total",
            "c) 30%",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 862,
        "Question": "Apache __________ is a data repository containing device information, images, and other relevant information for all sorts of mobile devices.",
        "Options": [
            "a) DirectMemory",
            "b) Directory",
            "c) DeviceMap",
            "d) Drill",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 863,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Drill is a build system based on Apache Ant and Apache Ivy",
            "b) DirectMemory’s main purpose is to act as a second-level cache",
            "c) Easyant is inspired by Google’s Dremel",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 864,
        "Question": "____________ is a secure and highly scalable micro sharing and micro-messaging platform.",
        "Options": [
            "a) ESME",
            "b) Directory",
            "c) Empire-db",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 865,
        "Question": "Which of the framework is used for building and consuming network services?",
        "Options": [
            "a) ESME",
            "b) DirectoryMap",
            "c) Empire-db",
            "d) Etch",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 866,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Felix is implementation of the OSGi R4 specification",
            "b) Falcon is a data processing and management solution",
            "c) Flex is application framework for building Flash-based applications",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 867,
        "Question": "_____________ is an open source system for expressive, declarative, fast, and efficient data analysis.",
        "Options": [
            "a) Flume",
            "b) Flink",
            "c) Flex",
            "d) ESME",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 868,
        "Question": "________________ is complete FTP Server based on Mina I/O system.",
        "Options": [
            "a) Giraph",
            "b) Gereition",
            "c) FtpServer",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 869,
        "Question": "_____________ is a distributed computing framework based on BSP.",
        "Options": [
            "a) HCataMan",
            "b) HCatlaog",
            "c) Hama",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 870,
        "Question": "Apache __________ is a generic cluster management framework used to build distributed systems.",
        "Options": [
            "a) Helix",
            "b) Gereition",
            "c) FtpServer",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 871,
        "Question": "The __________ data Mapper framework makes it easier to use a database with Java or .NET applications.",
        "Options": [
            "a) iBix",
            "b) Helix",
            "c) iBATIS",
            "d) iBAT",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 872,
        "Question": "Which of the following is java-based tool for tracking, resolving and managing project dependencies?",
        "Options": [
            "a) jclouds",
            "b) JDO",
            "c) ivy",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 873,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Jena is Java framework for building Semantic Web applications",
            "b) JSPWiki is Java-based wiki engine",
            "c) jUDDI is implementation of a Universal Description Discovery and Integration (UDDI) registry",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 874,
        "Question": "Which of the following are Content Management and publishing system based on Cocoon?",
        "Options": [
            "a) LibCloud",
            "b) Kafka",
            "c) Lenya",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 875,
        "Question": "__________ is used for Logging for .NET framework.",
        "Options": [
            "a) log4net",
            "b) logphp",
            "c) Lucene.NET",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 876,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Lucy is loose port of the Lucene search engine library, written in C and targeted at dynamic language users",
            "b) Manifold Connector Framework consist of connectors for content repositories like Sharepoint, Documentum, etc",
            "c) Marmotta is an open implementation of a Linked Data Platform",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 877,
        "Question": "__________ is a cluster manager that provides resource sharing and isolation across cluster applications.",
        "Options": [
            "a) Merlin",
            "b) Mesos",
            "c) Max",
            "d) Merge",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 878,
        "Question": "Which of the following is a data access framework?",
        "Options": [
            "a) Merge",
            "b) Lucene.NET",
            "c) MetaModel",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 879,
        "Question": "__________ is a library to support unit testing of Hadoop MapReduce jobs.",
        "Options": [
            "a) Myfaces",
            "b) Muse",
            "c) modftp",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 880,
        "Question": "Which of the following is a robust implementation of the OASIS WSDM?",
        "Options": [
            "a) Myfaces",
            "b) Muse",
            "c) modftp",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 881,
        "Question": "__________ is a framework for building Java Server application GUIs.",
        "Options": [
            "a) Myfaces",
            "b) Muse",
            "c) Flume",
            "d) BigTop",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 882,
        "Question": "Which of the following is a Web search software?",
        "Options": [
            "a) Imphala",
            "b) Nutch",
            "c) Oozie",
            "d) Manmgy",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 883,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) OFBiz stands for “The Open For Business Project”",
            "b) Od stands for “Orchestration Director Engine”",
            "c) OCG is Object-Graph Notation Language implementation in Java",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 884,
        "Question": "___________ defines an open application programming interface for common cloud application services.",
        "Options": [
            "a) Bigred",
            "b) Nuvem",
            "c) Oozie",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 885,
        "Question": "__________ is OData implementation in Java.",
        "Options": [
            "a) Bigred",
            "b) Nuvem",
            "c) Olingo",
            "d) Onami",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 886,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) OpenOffice.org is comprised of six personal productivity applications",
            "b) Open Climate Workbench is tool for scalable comparison of remote sensing observations",
            "c) OpenNLP is a machine learning based toolkit for the processing of natural language text",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 887,
        "Question": "___________ is an open source SQL query engine for Apache HBase.",
        "Options": [
            "a) Pig",
            "b) Phoenix",
            "c) Pivot",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 888,
        "Question": "___________ provides multiple language implementations of the Advanced Message Queuing Protocol (AMQP).",
        "Options": [
            "a) RTA",
            "b) Qpid",
            "c) RAT",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 889,
        "Question": "___________ is A WEb And SOcial Mashup Engine.",
        "Options": [
            "a) ServiceMix",
            "b) Samza",
            "c) Rave",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 890,
        "Question": "The ___________ project will create an ESB and component suite based on the Java Business Interface (JBI) standard – JSR 208.",
        "Options": [
            "a) ServiceMix",
            "b) Samza",
            "c) Rave",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 891,
        "Question": "Which of the following is a spatial information system?",
        "Options": [
            "a) Sling",
            "b) Solr",
            "c) SIS",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 892,
        "Question": "Stratos will be a polyglot _________ framework.",
        "Options": [
            "a) Daas",
            "b) PaaS",
            "c) Saas",
            "d) Raas",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 893,
        "Question": "Which of the following supports random-writable and advance-able sparse bitsets?",
        "Options": [
            "a) Stratos",
            "b) Kafka",
            "c) Sqoop",
            "d) Lucene",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 894,
        "Question": "____________ is an open-source version control system.",
        "Options": [
            "a) Stratos",
            "b) Kafka",
            "c) Sqoop",
            "d) Subversion",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 895,
        "Question": "___________ is a distributed data warehouse system for Hadoop.",
        "Options": [
            "a) Stratos",
            "b) Tajo",
            "c) Sqoop",
            "d) Lucene",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 896,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) TusCAN ia Service Component Architecture implementation",
            "b) Tob is a JSF based framework for web-applications",
            "c) Traffic is a scalable and extensible HTTP proxy server and cache",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 897,
        "Question": "___________ is a distributed, fault-tolerant, and high-performance realtime computation system.",
        "Options": [
            "a) Knife",
            "b) Storm",
            "c) Sqoop",
            "d) Lucene",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 898,
        "Question": "Which of the following is a standard compliant XML Query processor?",
        "Options": [
            "a) Whirr",
            "b) VXQuery",
            "c) Knife",
            "d) Lens",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 899,
        "Question": "Apache _________ is a project that enables development and consumption of REST style web services.",
        "Options": [
            "a) Wives",
            "b) Wink",
            "c) Wig",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 900,
        "Question": "__________ is a log collection and correlation software with reporting and alarming functionalities.",
        "Options": [
            "a) Lucene",
            "b) ALOIS",
            "c) Imphal",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 901,
        "Question": "__________ is a non-blocking, asynchronous, event driven high performance web framework.",
        "Options": [
            "a) AWS",
            "b) AWF",
            "c) AWT",
            "d) ASW",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 902,
        "Question": "___________ is the world’s most complete, tested, and popular distribution of Apache Hadoop and related projects.",
        "Options": [
            "a) MDH",
            "b) CDH",
            "c) ADH",
            "d) BDH",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 903,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Cloudera is also a sponsor of the Apache Software Foundation",
            "b) CDH is 100% Apache-licensed open source and is the only Hadoop solution to offer unified batch processing, interactive SQL, and interactive search, and role-based access controls",
            "c) More enterprises have downloaded CDH than all other such distributions combined",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 904,
        "Question": "Cloudera ___________ includes CDH and an annual subscription license (per node) to Cloudera Manager and technical support.",
        "Options": [
            "a) Enterprise",
            "b) Express",
            "c) Standard",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 905,
        "Question": "Cloudera Express includes CDH and a version of Cloudera ___________ lacking enterprise features such as rolling upgrades and backup/disaster recovery.",
        "Options": [
            "a) Enterprise",
            "b) Express",
            "c) Standard",
            "d) Manager",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 906,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) CDH contains the main, core elements of Hadoop",
            "b) In October 2012, Cloudera announced the Cloudera Impala project",
            "c) CDH may be downloaded from Cloudera’s website at no charge",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 907,
        "Question": "Cloudera Enterprise comes in ___________ edition.",
        "Options": [
            "a) One",
            "b) Two",
            "c) Three",
            "d) Four",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 908,
        "Question": "__________ is a online NoSQL developed by Cloudera.",
        "Options": [
            "a) HCatalog",
            "b) Hbase",
            "c) Imphala",
            "d) Oozie",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 909,
        "Question": "_______ is an open source set of libraries, tools, examples, and documentation engineered.",
        "Options": [
            "a) Kite",
            "b) Kize",
            "c) Ookie",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 910,
        "Question": "To configure short-circuit local reads, you will need to enable ____________ on local Hadoop.",
        "Options": [
            "a) librayhadoop",
            "b) libhadoop",
            "c) libhad",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 911,
        "Question": "CDH process and control sensitive data and facilitate __________",
        "Options": [
            "a) multi-tenancy",
            "b) flexibilty",
            "c) scalability",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 912,
        "Question": "Microsoft and Hortonworks joined their forces to make Hadoop available on ___________ for on-premise deployments.",
        "Options": [
            "a) Windows 7",
            "b) Windows Server",
            "c) Windows 8",
            "d) Ubuntu",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 913,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes",
            "b) GNU/Linux is supported as a development and production platform",
            "c) Distributed operation has not been well tested on Win32, so it is not supported as a production platform",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 914,
        "Question": "Hadoop ___________ is a utility to support running external map and reduce jobs.",
        "Options": [
            "a) Orchestration",
            "b) Streaming",
            "c) Collection",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 915,
        "Question": "In Hadoop _____________ go to the Hadoop distribution directory for HDInsight.",
        "Options": [
            "a) Shell",
            "b) Command Line",
            "c) Compaction",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 916,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) The other flavor of HDInsight interactive console is based on JavaScript",
            "b) Microsoft and Hortonworks have re-implemented the key binaries as executables",
            "c) The distribution consists of Hadoop 1.1.0, Pig-0.9.3, Hive 0.9.0, Mahout 0.5 and Sqoop 1.4.2",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 917,
        "Question": "Microsoft Azure HDInsight comes with __________ types of interactive console.",
        "Options": [
            "a) two",
            "b) three",
            "c) four",
            "d) five",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 918,
        "Question": "The key _________ command – which is traditionally a bash script – is also re-implemented as hadoop.cmd.",
        "Options": [
            "a) start",
            "b) hadoop",
            "c) had",
            "d) hadstrat",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 919,
        "Question": "Which of the following individual components are included on HDInsight clusters?",
        "Options": [
            "a) Hive",
            "b) Pig",
            "c) Oozie",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 920,
        "Question": "Microsoft .NET Library for Avro provides data serialization for the Microsoft ___________ environment.",
        "Options": [
            "a) .NET",
            "b) Hadoop",
            "c) Ubuntu",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 921,
        "Question": "Which of the following benefit is not a feature of HDInsight?",
        "Options": [
            "a) High availability",
            "b) High reliability",
            "c) High cost",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 922,
        "Question": "Amazon EMR also allows you to run multiple versions concurrently, allowing you to control your ___________ version upgrade.",
        "Options": [
            "a) Pig",
            "b) Windows Server",
            "c) Hive",
            "d) Ubuntu",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 923,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Amazon Elastic MapReduce (Amazon EMR) provides support for Apache Hive",
            "b) Pig extends the SQL paradigm by including serialization formats and the ability to invoke mapper and reducer scripts",
            "c) The Amazon Hive default input format is text",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 924,
        "Question": "The Amazon EMR default input format for Hive is __________",
        "Options": [
            "a) org.apache.hadoop.hive.ql.io.CombineHiveInputFormat",
            "b) org.apache.hadoop.hive.ql.iont.CombineHiveInputFormat",
            "c) org.apache.hadoop.hive.ql.io.CombineFormat",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 925,
        "Question": "Hadoop clusters running on Amazon EMR use ______ instances as virtual Linux servers for the master and slave nodes.",
        "Options": [
            "a) EC2",
            "b) EC3",
            "c) EC4",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 926,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Apache Hive saves Hive log files to /tmp/{user.name}/ in a file named hive.log",
            "b) Amazon EMR saves Hive logs to /mnt/var/log/apps/",
            "c) In order to support concurrent versions of Hive, the version of Hive you run determines the log file name",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 927,
        "Question": "Amazon EMR uses Hadoop processing combined with several __________  products.",
        "Options": [
            "a) AWS",
            "b) ASQ",
            "c) AMR",
            "d) AWES",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 928,
        "Question": "___________ is an RPC framework that defines a compact binary serialization format used to persist data structures for later analysis.",
        "Options": [
            "a) Pig",
            "b) Hive",
            "c) Thrift",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 929,
        "Question": "Impala on Amazon EMR requires _________ running Hadoop 2.x or greater.",
        "Options": [
            "a) AMS",
            "b) AMI",
            "c) AWR",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 930,
        "Question": "Impala executes SQL queries using a _________ engine.",
        "Options": [
            "a) MAP",
            "b) MPP",
            "c) MPA",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 931,
        "Question": "Amazon EMR clusters can read and process Amazon _________ streams directly.",
        "Options": [
            "a) Kinet",
            "b) kinematics",
            "c) Kinesis",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 932,
        "Question": "The Amazon ____________ is a Web-based service that allows business subscribers to run application programs in the Amazon.com computing environment.",
        "Options": [
            "a) EC3",
            "b) EC4",
            "c) EMR",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 933,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) Amazon Web Services offers reliable, scalable, and inexpensive cloud computing services",
            "b) MongoDB runs well on Amazon EC2",
            "c) To deploy MongoDB on EC2 you can either set up a new instance manually",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 934,
        "Question": "Amazon ___________ is a Web service that provides real-time monitoring to Amazon’s EC2 customers.",
        "Options": [
            "a) AmWatch",
            "b) CloudWatch",
            "c) IamWatch",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 935,
        "Question": "Amazon ___________ provides developers the tools to build failure resilient applications and isolate themselves from common failure scenarios.",
        "Options": [
            "a) EC2",
            "b) EC3",
            "c) EC4",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 936,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud",
            "b) Amazon EC2 is designed to make web-scale cloud computing easier for developers.",
            "c) Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction.",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 937,
        "Question": "Amazon EC2 provides virtual computing environments, known as __________",
        "Options": [
            "a) chunks",
            "b) instances",
            "c) messages",
            "d) none of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 938,
        "Question": "Amazon ___________ is well suited to transfer bulk amount of data.",
        "Options": [
            "a) EC2",
            "b) EC3",
            "c) EC4",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 939,
        "Question": "The EC2 can serve as a practically unlimited set of ___________ machines.",
        "Options": [
            "a) virtual",
            "b) real",
            "c) distributed",
            "d) all of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 940,
        "Question": "EC2 capacity can be increased or decreased in real time from as few as one to more than ___________ virtual machines simultaneously.",
        "Options": [
            "a) 1000",
            "b) 2000",
            "c) 3000",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 941,
        "Question": "AMI is uploaded to the Amazon _______ and registered with Amazon EC2, creating a so-called AMI identifier (AMI ID).",
        "Options": [
            "a) S2",
            "b) S3",
            "c) S4",
            "d) S5",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 942,
        "Question": "The IBM _____________ Platform provides all the foundational building blocks of trusted information, including data integration, data warehousing, master data management, big data and information governance.",
        "Options": [
            "a) InfoStream",
            "b) InfoSphere",
            "c) InfoSurface",
            "d) InfoData",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 943,
        "Question": "Point out the correct statement.",
        "Options": [
            "a) IBM InfoSphere DataStage is an ETL tool",
            "b) IBM InfoSphere DataStage is a part of the IBM Information Platforms Solutions suite and IBM InfoSphere",
            "c) InfoSphere uses a graphical notation to construct data integration solutions",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 944,
        "Question": "InfoSphere DataStage has __________ levels of Parallelism.",
        "Options": [
            "a) 1",
            "b) 2",
            "c) 3",
            "d) 4",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 945,
        "Question": "InfoSphere DataStage uses a client/server design where jobs are created and administered via a ________ client against a central repository on a server.",
        "Options": [
            "a) Ubuntu",
            "b) Windows",
            "c) Debian",
            "d) Solaris",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 946,
        "Question": "Point out the wrong statement.",
        "Options": [
            "a) InfoSphere DataStage also facilitates extended metadata management and enterprise connectivity",
            "b) Real-Time Integration pack can turn server or parallel jobs into SOA services",
            "c) In 2012 the suite was renamed to InfoSphere Information Server and the product was renamed to InfoSphere DataStage",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: c"
    },
    {
        "id": 947,
        "Question": "__________ is a name given to the version of DataStage that had a parallel processing architecture and parallel ETL jobs.",
        "Options": [
            "a) Enterprise Edition",
            "b) Server Edition",
            "c) MVS Edition",
            "d) TX",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 948,
        "Question": "___________ is used for processing complex transactions and messages.",
        "Options": [
            "a) PX",
            "b) Server Edition",
            "c) MVS Edition",
            "d) TX",
            ""
        ],
        "Answer": "Answer: d"
    },
    {
        "id": 949,
        "Question": "InfoSphere ___________ provides you with the ability to flexibly meet your unique information integration requirements.",
        "Options": [
            "a) Data Server",
            "b) Information Server",
            "c) Info Server",
            "d) All of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    },
    {
        "id": 950,
        "Question": "DataStage originated at __________ a company that developed two notable products: UniVerse database and the DataStage ETL tool.",
        "Options": [
            "a) VMark",
            "b) Vzen",
            "c) Hatez",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: a"
    },
    {
        "id": 951,
        "Question": "DataStage RTI is real time integration pack for __________",
        "Options": [
            "a) STD",
            "b) ISD",
            "c) EXD",
            "d) None of the mentioned",
            ""
        ],
        "Answer": "Answer: b"
    }
]